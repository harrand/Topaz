tz_gpu_appinfo ::= struct
{
	name : u8&;
	ver_maj : u32;
	ver_min : u32;
};

tz_gpu_err ::= enum
{
	.none := 0;
	.unknown := 1;
};

tz_gpu_hardware_type ::= enum
{
	.gpu := 0;
	.igpu := 1;
	.cpu := 2;
	.unknown := 3;
};

tz_gpu_hardware_caps ::= enum
{
	.graphics_compute := 0;
	.graphics_only := 1;
	.compute_only := 2;
	.none := 3;
};

tz_gpu_hardware ::= struct
{
	name : string;
	target_heap_gpu : u64;
	target_heap_cpu : u64;
	vram_size_mib : u64;
	type : tz_gpu_hardware_type;
	caps : tz_gpu_hardware_caps;
	id : u32;
	native_handle : u64;
};

tz_gpu_access ::= enum
{
	.static := 0;
	.dynamic := 1;
};

tz_gpu_resource ::= enum
{
	.invalid := -1;
};

tz_gpu_shader_sources ::= struct
{
	vertex_spv_data : u8 mut&;
	vertex_spv_count : u64;
	fragment_spv_data : u8 mut&;
	fragment_spv_count : u64;
};

tz_gpu_shader ::= enum
{
	.invalid := -1;
};

tz_gpu_cull ::= enum
{
	.back := 0;
	.front := 1;
	.none := 0;
};

tz_gpu_graphics_state ::= struct
{
	clear_colour : vec4;
	scissor : vec4u32;
	colour_targets_data : tz_gpu_resource&;
	colour_targets_count : u64;
	depth_target : tz_gpu_resource;
	index_buffer : tz_gpu_resource;
	draw_buffer : tz_gpu_resource;
	culling : tz_gpu_cull;
	static_tri_count : u64;
};

tz_gpu_compute_state ::= struct
{
	kernelx : u32;
	kernely : u32;
	kernelz : u32;
};

tz_gpu_pass_info ::= struct
{
	graphics : tz_gpu_graphics_state;
	compute : tz_gpu_compute_state;
	shader : tz_gpu_shader;
	resources_data : tz_gpu_resource&;
	resources_count : u64;
	name : u8&;
};

tz_gpu_pass ::= enum
{
	.invalid := -1;
};

valloc_initial_size : u64 static := (32 * 1024 * 1024); /*32 mib*/

valloc_t ::= struct
{
	device_mem : u64;
	size : u64;
	cursor : u64;
};

vallocator_t ::= struct
{
	buffer_gpu : valloc_t;
	image_gpu : valloc_t;
	buffer_cpu : valloc_t;
};

frame_data ::= struct
{
	cpool : u64;
	cmds : u64;

	swapchain_fence : u64;
	swapchain_sem : u64;
};

scratch_data ::= struct
{
	cpool : u64;
	cmds : u64;
	fence : u64;
};

// globals.
MAX_GLOBAL_IMAGE_COUNT ::= 8192;
MAX_IMAGE_COUNT_PER_PASS ::= 4096;

[[private]]
vallocator : vallocator_t mut;
[[private]]
scratch : scratch_data mut;
[[private]]
frame_overlap ::= 2;
frames : frame_data mut#2;
[[private]]
set_layouts : u64 mut#2;
[[private]]
vkinst : u64 mut;
[[private]]
used_device : u64 mut;
[[private]]
used_hardware : u64 mut;
[[private]]
used_qfi : u32 mut;
[[private]]
used_mti_gpu : u32 mut;
[[private]]
used_mti_cpu : u32 mut;
[[private]]
graphics_queue : u64 mut;
[[private]]
compute_queue : u64 mut;

[[private]]
pipeline_layout : u64 mut;

[[private]]
surface : u64 mut := 0;
[[private]]
swapchain : u64 mut := 0;
[[private]]
swapchain_width : u64 mut := 0;
[[private]]
swapchain_height : u64 mut := 0;

resource_data_t ::= struct
{
	is_buffer : bool;
	vk_handle : u64;
	buffer_device_address : u64;
};
resources : resource_data_t mut& mut;
resource_count : u64 mut := 0;
resource_cap : u64 mut := 0;

shader_data_t ::= struct
{
	is_graphics : bool;
	vertex_module : u64;
	fragment_module : u64;
	compute_module : u64;
};

shaders : shader_data_t mut& mut;
shaders_count : u64 mut := 0;
shaders_cap : u64 mut := 0;

// internal pass data.
pass_data_t ::= struct
{
	info : tz_gpu_pass_info;
	metabuf : u64;
	is_compute : bool;
	pipeline : u64;
};

passes : pass_data_t mut& mut;
passes_count : u64 mut := 0;
passes_cap : u64 mut := 0;

// implementation details

[[private]]
impl_shader_is_compute ::= func(shader : tz_gpu_shader) -> bool
{
	shadptr ::= (shaders at (shader@s64));
	return (shadptr->compute_module) != 0;
};

[[private]]
impl_create_compute_pipeline ::= func(shader : tz_gpu_shader) -> u64
{
	shadptr ::= (shaders at (shader@s64));
	compute_shader_module ::= shadptr->compute_module;
	if(compute_shader_module == 0)
	{
		puts("invalid compute shader");
		__debugbreak();
	}
	create ::= VkComputePipelineCreateInfo
	{
		.sType := 29;
		.pNext := null;
		.flags := 0;
		.stage := VkPipelineShaderStageCreateInfo
		{
			.sType := 18;
			.pNext := null;
			.flags := 0;
			.stage := 0x00000020; 
			.module := compute_shader_module;
			.pName := "main";
			.pSpecializationInfo := null;
		};
		.layout := pipeline_layout;
		.basePipelineHandle := 0;
		.basePipelineIndex := -1;
	};
	ret : u64 mut;

	vk_check(vk.create_compute_pipelines(used_device, 0, 1, ref create, null, ref ret));
	return ret;
};

[[private]]
impl_alloc_new_pass ::= func(a : arena mut&) -> pass_data_t mut&
{
	if(passes_cap == 0)
	{
		passes = arena_push(a, __sizeof(pass_data_t) * 32);
		passes_cap = 32;
	}
	if(passes_count > passes_cap)
	{
		puts("ran out of pass capacity. todo: fix this.");
		__debugbreak();
	}
	id ::= passes_count;
	passes_count = passes_count + 1;
	return passes at id;
};

[[private]]
impl_alloc_new_shader ::= func(a : arena mut&) -> shader_data_t mut&
{
	if(shaders_cap == 0)
	{
		shaders = arena_push(a, __sizeof(shader_data_t) * 32);
		shaders_cap = 32;
	}
	if(shaders_count > shaders_cap)
	{
		puts("ran out of shader capacity. todo: fix this.");
		__debugbreak();
	}
	id ::= shaders_count;
	shaders_count = shaders_count + 1;
	return shaders at id;
};

[[private]]
impl_alloc_new_resource ::= func(a : arena mut&) -> resource_data_t mut&
{
	if(resource_cap == 0)
	{
		resources = arena_push(a, __sizeof(resource_data_t) * 1024);
		resource_cap = 1024;
	}
	if(resource_count > resource_cap)
	{
		puts("ran out of resources. todo: fix this.");
		__debugbreak();
	}
	id ::= resource_count;
	resource_count = resource_count + 1;
	return resources at id;
};

[[private]]
impl_vallocator_initial_setup ::= func() -> v0
{
	bda_flags ::= VkMemoryAllocateFlagsInfo
	{
		.sType := 1000060000;
		.pNext := null;
		.flags := 0x00000002;
		.deviceMask := 0;
	};
	gpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := used_mti_gpu;
	};
	cpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := used_mti_cpu;
	};

	// create initial allocations for each case (buffer gpu, image gpu, buffer cpu).
	bgpu ::= ref(vallocator.buffer_gpu);
	vk_check(vk.allocate_memory(used_device, ref gpucreate, null, ref (bgpu->device_mem)));
	(bgpu->size) = valloc_initial_size;
	(bgpu->cursor) = 0;
	igpu ::= ref(vallocator.image_gpu);
	vk_check(vk.allocate_memory(used_device, ref gpucreate, null, ref (igpu->device_mem)));
	(igpu->size) = valloc_initial_size;
	(igpu->cursor) = 0;

	bcpu ::= ref(vallocator.buffer_cpu);
	vk_check(vk.allocate_memory(used_device, ref cpucreate, null, ref (bcpu->device_mem)));
	(bcpu->size) = valloc_initial_size;
	(bcpu->cursor) = 0;
};

[[private]]
impl_vallocator_free_all ::= func() -> v0
{
	bgpu ::= vallocator.buffer_gpu;
	vk.free_memory(used_device, bgpu.device_mem, null);
	igpu ::= vallocator.image_gpu;
	vk.free_memory(used_device, igpu.device_mem, null);

	bcpu ::= vallocator.buffer_cpu;
	vk.free_memory(used_device, bcpu.device_mem, null);
};

[[impl]]
impl_bind_buffer_mem ::= func(buffer : u64, bufsize : u64, gpu_memory : bool) -> v0
{
	valloc : valloc_t mut& mut := ref (vallocator.buffer_cpu);
	if(gpu_memory)
	{
		valloc = ref (vallocator.buffer_gpu);
	}
	if((valloc->cursor + bufsize) > (valloc->size))
	{
		puts("allocation too large. requested ");
		putuint(valloc->cursor + bufsize);
		puts("B but there was only ");
		putuint(valloc->size);
		puts("B remaining.");
		__debugbreak();
	}
	ret ::= vk.bind_buffer_memory(used_device, buffer, valloc->device_mem, valloc->cursor);
	if(ret == -1) /* out of host memory */
	{
		puts("oom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) /* out of device memory */
	{
		puts("voom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	// todo: align safely.
	valloc->cursor = (valloc->cursor) + bufsize;
};

[[impl]]
impl_bind_image_mem ::= func(image : u64, imgsize : u64) -> v0
{
	valloc : valloc_t mut& mut := ref (vallocator.image_gpu);
	if((valloc->cursor + imgsize) > (valloc->size))
	{
		puts("allocation too large. requested ");
		putuint(valloc->cursor + imgsize);
		puts("B but there was only ");
		putuint(valloc->size);
		puts("B remaining.");
		__debugbreak();
	}
	ret ::= vk.bind_image_memory(used_device, image, valloc->device_mem, valloc->cursor);
	if(ret == -1) /* out of host memory */
	{
		puts("oom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) /* out of device memory */
	{
		puts("voom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	// todo: align safely.
	valloc->cursor = (valloc->cursor) + imgsize;
};

[[private]]
impl_initialise_pipeline_layout ::= func() -> u64
{
	bindings : VkDescriptorSetLayoutBinding mut#2;
	deref(bindings at 0) = VkDescriptorSetLayoutBinding
	{
		.binding := 0;
		.descriptorType := 7; /*storage buffer*/
		.descriptorCount := 1;
		.stageFlags := 0x7FFFFFFF;
		.pImmutableSamplers := null;
	};
	deref(bindings at 1) = VkDescriptorSetLayoutBinding
	{
		.binding := 1;
		.descriptorType := 1; /*combined image sampler*/
		.descriptorCount := MAX_GLOBAL_IMAGE_COUNT@u32;
		.stageFlags := 0x7FFFFFFF;
		.pImmutableSamplers := null;
	};
	bindings_size : u32 := __sizeof(bindings) / __sizeof(deref(bindings at 0));

	flags : s32 mut#2;
	deref(flags at 0) = 0;
	deref(flags at 1) =
		(0x00000004 | /*VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT*/ 
		0x00000001 | /*VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT*/
		0x00000002 | /*VK_DESCRIPTOR_BINDING_UPDATE_UNUSED_WHILE_PENDING_BIT*/
		0x00000008); /*VK_DESCRIPTOR_BINDING_VARIABLE_DESCRIPTOR_COUNT_BIT*/
	flags_size : u32 := __sizeof(flags) / __sizeof(deref(flags at 0));

	flags_create ::= VkDescriptorSetLayoutBindingFlagsCreateInfo
	{
		.sType := 1000161000;
		.pNext := null;
		.bindingCount := flags_size@u32;
		.pBindingFlags := flags at 0;
	};

	layout_create ::= VkDescriptorSetLayoutCreateInfo
	{
		.sType := 32;
		.pNext := (ref flags_create)@_;
		.flags := 0x00000002; /*VK_DESCRIPTOR_SET_LAYOUT_CREATE_UPDATE_AFTER_BIND_POOL_BIT */
		.bindingCount := bindings_size@u32;
		.pBindings := bindings at 0;
	};

	counter : u64 mut := 0;
	while(counter < frame_overlap)
	{
		vk_check(vk.create_descriptor_set_layout(used_device, ref layout_create, null, set_layouts at counter));
		counter = counter + 1;
	}

	create ::= VkPipelineLayoutCreateInfo
	{
		.sType := 30;
		.pNext := null;
		.flags := 0;
		.setLayoutCount := frame_overlap@u32;
		.pSetLayouts := set_layouts at 0;
		.pushConstantRangeCount := 0;
		.pPushConstantRanges := null;
	};

	ret : u64 mut;
	vk_check(vk.create_pipeline_layout(used_device, ref create, null, ref ret));

	return ret;
};

[[private]]
impl_need_swapchain ::= func(w : u64, h : u64, wnd : window_handle) -> VkResult
{
	if(surface == 0)
	{
		// create surface.
		if static(__is_windows)
		{
			create ::= VkWin32SurfaceCreateInfoKHR
			{
				.sType := 1000009000;
				.pNext := null;
				.flags := 0;
				.hinstance := GetModuleHandleA(null);
				.hwnd := wnd@s64@u64;
			};
			vk_check(vk.create_win32_surface_khr(vkinst, ref create, null, ref surface));
		}
		if static(__is_linux)
		{
			__error("linux vulkan support NYI");
		}
	}

	if(swapchain_width@_ == w)
	{
		if(swapchain_height@_ == h)
		{
			return VkResult.VK_SUCCESS;
		}
	}

	caps : VkSurfaceCapabilitiesKHR mut;
	vk.get_physical_device_surface_capabilities_khr(used_hardware, surface, ref caps);

	swch_create ::= VkSwapchainCreateInfoKHR
	{
		.sType := 1000001000;
		.pNext := null;
		.flags := 0;
		.surface := surface;
		.minImageCount := 2;
		.imageFormat := 44;
		.imageColorSpace := 0;
		.imageExtent := VkExtent2D{.width := 800; .height := 600;};
		.imageArrayLayers := 1;
		.imageUsage := (0x00000010 | 0x00000002);
		.imageSharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
		.preTransform := caps.currentTransform;
		.compositeAlpha := 0x00000001;
		// 0 = immediate. fifo (vsync) = 2
		.presentMode := 0;
		.clipped := 0;
		.oldSwapchain := swapchain;
	};
	old_swapchain ::= swapchain;
	vk_check(vk.create_swapchain_khr(used_device, ref swch_create, null, ref swapchain));

	// get physical device.
	// create swapchain.
	// create swapchain images and views.
	// set swapchain_width and height to the new values.
	// create system image and system depth image.

	return VkResult.VK_SUCCESS;
};

[[private]]
impl_force_new_swapchain ::= func(wnd : window_handle) -> VkResult
{
	w ::= swapchain_width;
	h ::= swapchain_height;
	swapchain_width = 0;
	swapchain_height = 0;
	return impl_need_swapchain(w, h, wnd);
};


[[private]]
impl_fill_hardware_info ::= func(hw : tz_gpu_hardware mut&, pdev : u64, a : arena mut&) -> v0
{
	props : VkPhysicalDeviceProperties mut;
	vk.get_physical_device_properties(pdev, ref props);
	memprops : VkPhysicalDeviceMemoryProperties mut;
	vk.get_physical_device_memory_properties(pdev, ref memprops);

	(hw->id) = -1;
	(hw->native_handle) = pdev;
	(hw->vram_size_mib) = 0;
	(hw->target_heap_gpu) = 0;
	(hw->target_heap_cpu) = 0;

	(hw->type) = (tz_gpu_hardware_type.unknown);
	if(props.deviceType == 0x01)
	{
		(hw->type) = (tz_gpu_hardware_type.igpu);
	}
	if(props.deviceType == 0x02)
	{
		(hw->type) = (tz_gpu_hardware_type.gpu);
	}
	if(props.deviceType == 0x04)
	{
		(hw->type) = (tz_gpu_hardware_type.cpu);
	}
	counter : u32 mut := 0;

	cur_vk_heap : VkMemoryHeap mut;
	cur_mem_ty : VkMemoryType mut;

	max_mem : u64 mut := 0;
	size : u64 mut := 0;
	while(counter < (memprops.memoryHeapCount))
	{
		cur_vk_heap = deref(memprops.memoryHeaps at counter);
		cur_mem_ty = deref(memprops.memoryTypes at counter);

		size = 0;
		if(cur_vk_heap.flags & 1) /* gpu memory */
		{
			size = (cur_vk_heap.size) / (1024 * 1024);
		}
		if(cur_vk_heap.flags == 0) /* not gpu memory */
		{
			(hw->target_heap_cpu) = counter@_;
		}
		if(size > max_mem)
		{
			max_mem = size;
			(hw->target_heap_gpu) = counter@_;
		}
		(hw->vram_size_mib) = (hw->vram_size_mib) + size;

		counter = counter + 1;
	}

	family_count : u32 mut := 0;
	vk.get_physical_device_queue_family_properties(pdev, ref family_count, null);
	families : VkQueueFamilyProperties mut& := arena_push(a, __sizeof(VkQueueFamilyProperties) * family_count);
	vk.get_physical_device_queue_family_properties(pdev, ref family_count, families);

	counter = 0;
	cur_fam : VkQueueFamilyProperties mut;
	while(counter < family_count)
	{
		// hw->idx should be set to the first graphics compute queue.
		cur_fam = deref(families at counter);
		if((cur_fam.queueFlags) & 0x03)
		{
			(hw->id) = counter;
			counter = family_count;
		}
		counter = counter + 1;
	}

	(hw->name) = create_string(props.deviceName at 0, a);
};

// api

tz_gpu_init ::= func(info : tz_gpu_appinfo) -> v0
{
	TZ_GPU_VERSION ::= VK_MAKE_API_VERSION(0, 0, 1, 0);
	vulkan_init();

	appinfo ::= VkApplicationInfo
	{
		.sType := 0;
		.pNext := null;
		.pApplicationName := info.name;
		.applicationVersion := VK_MAKE_API_VERSION(0, info.ver_maj, info.ver_min, 0);
		.pEngineName := "tz_gpu";
		.engineVersion := TZ_GPU_VERSION;
		.apiVersion := VK_MAKE_API_VERSION(0, 1, 3, 0);
	};

	extensions : u8& mut#3;
	deref(extensions at 0) = "VK_KHR_surface";
	if static(__is_windows)
	{
		deref(extensions at 1) = "VK_KHR_win32_surface";
	}
	if static(__is_linux)
	{
		deref(extensions at 1) = "VK_KHR_xlib_surface";
	}
	// todo: make VK_EXT_debug_utils optional.
	deref(extensions at 2) = "VK_EXT_debug_utils";
	extension_count : u32 := __sizeof(extensions) / __sizeof(u8&);

	layers : u8& mut#1;
	deref(layers at 0) = "VK_LAYER_KHRONOS_validation";

	create ::= VkInstanceCreateInfo
	{
		.sType := 1;
		.pNext := null;
		.flags := 0;
		.pApplicationInfo := ref appinfo;
		.enabledLayerCount := 1;
		.ppEnabledLayerNames := layers at 0;
		.enabledExtensionCount := extension_count;
		.ppEnabledExtensionNames := extensions at 0;
	};

	vk_check(vk.create_instance(ref create, null, ref vkinst));
	vulkan_load_functions(vkinst);
};

tz_gpu_term ::= func() -> v0
{
	if(pipeline_layout != 0)
	{
		vk.destroy_pipeline_layout(used_device, pipeline_layout, null);
		pipeline_layout = 0;
	}
	if(scratch.cpool != 0)
	{
		vk.destroy_command_pool(used_device, scratch.cpool, null);
		scratch.cpool = 0;
	}
	if(scratch.fence != 0)
	{
		vk.destroy_fence(used_device, scratch.fence, null);
	}
	counter : u64 mut := 0;
	frame_ptr : frame_data mut& mut;
	while(counter < frame_overlap)
	{
		frame_ptr = (frames at counter);
		if(frame_ptr->cpool != 0)
		{
			vk.destroy_command_pool(used_device, frame_ptr->cpool, null);
			(frame_ptr->cpool) = 0;
		}
		if(frame_ptr->swapchain_fence != 0)
		{
			vk.destroy_fence(used_device, frame_ptr->swapchain_fence, null);
			(frame_ptr->swapchain_fence) = 0;
		}
		if(frame_ptr->swapchain_sem != 0)
		{
			vk.destroy_semaphore(used_device, frame_ptr->swapchain_sem, null);
			(frame_ptr->swapchain_sem) = 0;
		}

		// dont forget descriptor layouts.
		vk.destroy_descriptor_set_layout(used_device, deref(set_layouts at counter), null);
		deref(set_layouts at counter) = 0;

		counter = counter + 1;
	}
	if(swapchain != 0)
	{
		vk.destroy_swapchain_khr(used_device, swapchain, null);
	}

	// destroy all passes
	counter = 0;
	passptr : pass_data_t mut& mut;
	while(counter < passes_count)
	{
		passptr = passes at counter;	
		vk.destroy_buffer(used_device, passptr->metabuf, null);
		counter = counter + 1;
	}
	passes_count = 0;

	// destroy all resources.
	counter = 0;
	resptr : resource_data_t mut& mut;
	while(counter < resource_count)
	{
		resptr = resources at counter;	
		if(resptr->is_buffer)
		{
			vk.destroy_buffer(used_device, resptr->vk_handle, null);
			(resptr->vk_handle) = 0;
		}
		if(!(resptr->is_buffer))
		{
			puts("errtodo: delete image resources.");
			__debugbreak();
		}
		counter = counter + 1;
	}
	resource_count = 0;

	// destroy all shaders.
	counter = 0;
	shad : shader_data_t mut& mut;
	while(counter < shaders_count)
	{
		shad = shaders at counter;	
		if(shad->vertex_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->vertex_module, null);
			shad->vertex_module = 0;
		}
		if(shad->fragment_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->fragment_module, null);
			shad->fragment_module = 0;
		}
		if(shad->compute_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->compute_module, null);
			shad->compute_module = 0;
		}
		counter = counter + 1;
	}
	shaders_count = 0;

	impl_vallocator_free_all();

	vk.destroy_device(used_device, null);
	if(surface != 0)
	{
		vk.destroy_surface_khr(vkinst, surface, null);
	}
	vk.destroy_instance(vkinst, null);
	vulkan_term();
};

tz_gpu_hardware_count ::= func() -> u64
{
	ret : u32 mut;
	vk_check(vk.enumerate_physical_devices(vkinst, ref ret, null));
	return ret@_;
};

tz_gpu_iterate_hardware ::= func(array : tz_gpu_hardware mut&, array_size : u64, a : arena mut&) -> tz_gpu_err
{
	// need an array of VkPhysicalDevices for vulkan to write into.
pdev_storage : u64 mut& := arena_push(a, __sizeof(u64) * array_size);
	physical_device_count : u32 mut := array_size@_;
	result ::= vk.enumerate_physical_devices(vkinst, ref physical_device_count, pdev_storage);
	if(result != 0)
	{
		return tz_gpu_err.unknown;
	}
	
	counter : u64 mut := 0;
	while(counter < array_size)
	{
		impl_fill_hardware_info(array at counter, deref(pdev_storage at counter), a);
		counter = (counter + 1);
	}
	return tz_gpu_err.none;
};

tz_gpu_use_hardware ::= func(hardware : tz_gpu_hardware) -> v0
{
	pdev ::= hardware.native_handle;
	queue_priority : f32 := 1.0;

	qcreate ::= VkDeviceQueueCreateInfo
	{
		.sType := 2;
		.pNext := null;
		.flags := 0;
		.queueFamilyIndex := hardware.id;
		.queueCount := 1;
		.pQueuePriorities := ref queue_priority;
	};

	features13 ::= VkPhysicalDeviceVulkan13Features
	{
		.sType := 53;
		.pNext := null;
		.robustImageAccess := 0;
		.inlineUniformBlock := 0;
		.descriptorBindingInlineUniformBlockUpdateAfterBind := 0;
		.pipelineCreationCacheControl := 0;
		.privateData := 0;
		.shaderDemoteToHelperInvocation := 0;
		.shaderTerminateInvocation := 0;
		.subgroupSizeControl := 0;
		.computeFullSubgroups := 0;
		.synchronization2 := 1;
		.textureCompressionASTC_HDR := 0;
		.shaderZeroInitializeWorkgroupMemory := 0;
		.dynamicRendering := 1;
		.shaderIntegerDotProduct := 0;
		.maintenance4 := 0;
	};

	features12 ::= VkPhysicalDeviceVulkan12Features 
	{
		.sType := 51;
		.pNext := (ref features13)@_;
		.samplerMirrorClampToEdge := 0;
		.drawIndirectCount := 1;
		.storageBuffer8BitAccess := 0;
		.uniformAndStorageBuffer8BitAccess := 0;
		.storagePushConstant8 := 0;
		.shaderBufferInt64Atomics := 0;
		.shaderSharedInt64Atomics := 0;
		.shaderFloat16 := 0;
		.shaderInt8 := 0;
		.descriptorIndexing := 1;
		.shaderInputAttachmentArrayDynamicIndexing := 0;
		.shaderUniformTexelBufferArrayDynamicIndexing := 0;
		.shaderStorageTexelBufferArrayDynamicIndexing := 0;
		.shaderUniformBufferArrayNonUniformIndexing := 0;
		.shaderSampledImageArrayNonUniformIndexing := 1;
		.shaderStorageBufferArrayNonUniformIndexing := 0;
		.shaderStorageImageArrayNonUniformIndexing := 0;
		.shaderInputAttachmentArrayNonUniformIndexing := 0;
		.shaderUniformTexelBufferArrayNonUniformIndexing := 0;
		.shaderStorageTexelBufferArrayNonUniformIndexing := 0;
		.descriptorBindingUniformBufferUpdateAfterBind := 0;
		.descriptorBindingSampledImageUpdateAfterBind := 1;
		.descriptorBindingStorageImageUpdateAfterBind := 0;
		.descriptorBindingStorageBufferUpdateAfterBind := 0;
		.descriptorBindingUniformTexelBufferUpdateAfterBind := 0;
		.descriptorBindingStorageTexelBufferUpdateAfterBind := 0;
		.descriptorBindingUpdateUnusedWhilePending := 1;
		.descriptorBindingPartiallyBound := 1;
		.descriptorBindingVariableDescriptorCount := 1;
		.runtimeDescriptorArray := 1;
		.samplerFilterMinmax := 0;
		.scalarBlockLayout := 0;
		.imagelessFramebuffer := 0;
		.uniformBufferStandardLayout := 0;
		.shaderSubgroupExtendedTypes := 0;
		.separateDepthStencilLayouts := 0;
		.hostQueryReset := 0;
		.timelineSemaphore := 1;
		.bufferDeviceAddress := 1;
		.bufferDeviceAddressCaptureReplay := 0;
		.bufferDeviceAddressMultiDevice := 0;
		.vulkanMemoryModel := 0;
		.vulkanMemoryModelDeviceScope := 0;
		.vulkanMemoryModelAvailabilityVisibilityChains := 0;
		.shaderOutputViewportIndex := 0;
		.shaderOutputLayer := 0;
		.subgroupBroadcastDynamicId := 0;
	};

	enabled_features ::= VkPhysicalDeviceFeatures2
	{
		.sType := 1000059000;
		.pNext := (ref features12)@_;
		.features := VkPhysicalDeviceFeatures
		{
			.robustBufferAccess := 0;
			.fullDrawIndexUint32 := 0;
			.imageCubeArray := 0;
			.independentBlend := 0;
			.geometryShader := 0;
			.tessellationShader := 0;
			.sampleRateShading := 0;
			.dualSrcBlend := 0;
			.logicOp := 0;
			.multiDrawIndirect := 0;
			.drawIndirectFirstInstance := 0;
			.depthClamp := 0;
			.depthBiasClamp := 0;
			.fillModeNonSolid := 0;
			.depthBounds := 0;
			.wideLines := 0;
			.largePoints := 0;
			.alphaToOne := 0;
			.multiViewport := 0;
			.samplerAnisotropy := 0;
			.textureCompressionETC2 := 0;
			.textureCompressionASTC_LDR := 0;
			.textureCompressionBC := 0;
			.occlusionQueryPrecise := 0;
			.pipelineStatisticsQuery := 0;
			.vertexPipelineStoresAndAtomics := 0;
			.fragmentStoresAndAtomics := 0;
			.shaderTessellationAndGeometryPointSize := 0;
			.shaderImageGatherExtended := 0;
			.shaderStorageImageExtendedFormats := 0;
			.shaderStorageImageMultisample := 0;
			.shaderStorageImageReadWithoutFormat := 0;
			.shaderStorageImageWriteWithoutFormat := 0;
			.shaderUniformBufferArrayDynamicIndexing := 0;
			.shaderSampledImageArrayDynamicIndexing := 0;
			.shaderStorageBufferArrayDynamicIndexing := 0;
			.shaderStorageImageArrayDynamicIndexing := 0;
			.shaderClipDistance := 0;
			.shaderCullDistance := 0;
			.shaderFloat64 := 0;
			.shaderInt64 := 0;
			.shaderInt16 := 0;
			.shaderResourceResidency := 0;
			.shaderResourceMinLod := 0;
			.sparseBinding := 0;
			.sparseResidencyBuffer := 0;
			.sparseResidencyImage2D := 0;
			.sparseResidencyImage3D := 0;
			.sparseResidency2Samples := 0;
			.sparseResidency4Samples := 0;
			.sparseResidency8Samples := 0;
			.sparseResidency16Samples := 0;
			.sparseResidencyAliased := 0;
			.variableMultisampleRate := 0;
			.inheritedQueries := 0;
		};
	};

	extensions : u8& mut#1;
	deref(extensions at 0) = "VK_KHR_swapchain";
	extension_count : u32 := __sizeof(extensions) / __sizeof(u8&);

	create ::= VkDeviceCreateInfo
	{
		.sType := 3;
		.pNext := (ref enabled_features)@_;
		.flags := 0;
		.queueCreateInfoCount := 1;
		.pQueueCreateInfos := (ref qcreate)@_;
		.enabledLayerCount := 0;
		.ppEnabledLayerNames := null;
		.enabledExtensionCount := extension_count;
		.ppEnabledExtensionNames := extensions at 0;
		.pEnabledFeatures := null;
	};

	vk_check(vk.create_device(pdev, ref create, null, ref used_device));
	used_hardware = pdev;
	used_qfi = hardware.id;
	used_mti_gpu = hardware.target_heap_gpu;
	used_mti_cpu = hardware.target_heap_cpu;

	vk.get_device_queue(used_device, used_qfi, 0, ref graphics_queue);
	vk.get_device_queue(used_device, used_qfi, 0, ref compute_queue);

	pool_create ::= VkCommandPoolCreateInfo
	{
		.sType := 39;
		.pNext := null;
		.flags := 0x02;
		.queueFamilyIndex := used_qfi;
	};
	
	counter : u64 mut := 0;
	frame_ptr : frame_data mut& mut;

	cmd_info : VkCommandBufferAllocateInfo mut := VkCommandBufferAllocateInfo
	{
		.sType := 40;
		.pNext := null;
		.commandPool := 0;
		.level := 0;
		.commandBufferCount := 1;
	};

	fence_create ::= VkFenceCreateInfo
	{
		.sType := 8;
		.pNext := null;
		.flags := 0;
	};

	sem_create ::= VkSemaphoreCreateInfo
	{
		.sType := 9;
		.pNext := null;
		.flags := 0;
	};

	while(counter < frame_overlap)
	{
		frame_ptr = (frames at counter);
		vk_check(vk.create_command_pool(used_device, ref pool_create, null, ref (frame_ptr->cpool)));
		cmd_info.commandPool = (frame_ptr->cpool);
		vk_check(vk.allocate_command_buffers(used_device, ref cmd_info, ref (frame_ptr->cmds)));

		vk_check(vk.create_fence(used_device, ref fence_create, null, ref (frame_ptr->swapchain_fence)));
		vk_check(vk.create_semaphore(used_device, ref sem_create, null, ref (frame_ptr->swapchain_sem)));

		counter = counter + 1;
	}

	vk_check(vk.create_command_pool(used_device, ref pool_create, null, ref (scratch.cpool)));
	cmd_info.commandPool = scratch.cpool;
	vk_check(vk.allocate_command_buffers(used_device, ref cmd_info, ref (scratch.cmds)));

	vk_check(vk.create_fence(used_device, ref fence_create, null, ref (scratch.fence)));

	pipeline_layout = impl_initialise_pipeline_layout();
	impl_vallocator_initial_setup();

	puts("using ");
	print_string(ref (hardware.name));
	putchar(10);
};

tz_gpu_create_buffer ::= func(size : u64, access : tz_gpu_access, a : arena mut&) -> tz_gpu_resource
{
	retid ::= resource_count;
	resptr ::= impl_alloc_new_resource(a);
	resptr->is_buffer = true;
	create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := null;
		.flags := 0;
		.size := size;
		.usage := (0x00000020 | 0x00020000);
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
	};
	vk_check(vk.create_buffer(used_device, ref create, null, ref (resptr->vk_handle)));
	bda ::= VkBufferDeviceAddressInfo
	{
		.sType := 1000244001;
		.pNext := null;
		.buffer := resptr->vk_handle;
	};
	impl_bind_buffer_mem(resptr->vk_handle, size, access == (tz_gpu_access.static));
	resptr->buffer_device_address = vk.get_buffer_device_address(used_device, ref bda);
	return retid@tz_gpu_resource;
};

tz_gpu_load_shader_files ::= func(vertex_spv_path : u8&, fragment_spv_path : u8&, a : arena mut&) -> tz_gpu_shader_sources
{
	ret : tz_gpu_shader_sources mut;
	ret.vertex_spv_count = file_size_bytes(vertex_spv_path);
	ret.fragment_spv_count = file_size_bytes(fragment_spv_path);

	ret.vertex_spv_data = arena_push(a, ret.vertex_spv_count);
	ret.fragment_spv_data = arena_push(a, ret.fragment_spv_count);

	file_read(vertex_spv_path, ret.vertex_spv_data, ret.vertex_spv_count);
	file_read(fragment_spv_path, ret.fragment_spv_data, ret.fragment_spv_count);

	return ret;
};

tz_gpu_create_graphics_shader ::= func(vertex_source : u8&, vertex_source_len : u64, fragment_source : u8&, fragment_source_len : u64, a : arena mut&) -> tz_gpu_shader
{
	retid ::= shaders_count;
	resptr ::= impl_alloc_new_shader(a);
	resptr->is_graphics = true;
	resptr->compute_module = 0;

	vcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := null;
		.flags := 0;
		.codeSize := vertex_source_len@_;
		.pCode := vertex_source@_;
	};
	vk.create_shader_module(used_device, ref vcreate, null, ref (resptr->vertex_module));
	fcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := null;
		.flags := 0;
		.codeSize := fragment_source_len@_;
		.pCode := fragment_source@_;
	};
	vk.create_shader_module(used_device, ref fcreate, null, ref (resptr->fragment_module));

	return retid@tz_gpu_shader;
};

tz_gpu_create_compute_shader ::= func(compute_source : u8&, compute_source_len : u64, a : arena mut&) -> tz_gpu_shader
{
	retid ::= shaders_count;
	resptr ::= impl_alloc_new_shader(a);
	resptr->is_graphics = false;
	resptr->vertex_module = 0;
	resptr->fragment_module = 0;

	vcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := null;
		.flags := 0;
		.codeSize := compute_source_len@_;
		.pCode := compute_source@_;
	};
	vk.create_shader_module(used_device, ref vcreate, null, ref (resptr->compute_module));

	return retid@tz_gpu_shader;
};

tz_gpu_create_pass ::= func(info : tz_gpu_pass_info, a : arena mut&) -> tz_gpu_pass
{
	retid ::= passes_count;
	passptr ::= impl_alloc_new_pass(a);
	passptr->info = info;
	// first thing we need is a metabuffer.
	// this is a non-BDA buffer that contains all the BDA addresses.
	// let's figure out how many buffer resources we have
	buffer_rescount : u64 mut := 0;
	counter : u64 mut := 0;
	cur_resource : resource_data_t mut;
	while(counter < (info.resources_count))
	{
		cur_resource = deref (resources at (deref(info.resources_data at counter)@s64));
		if(cur_resource.is_buffer)
		{
			buffer_rescount = buffer_rescount + 1;
		}
		counter = counter + 1;
	}

	size : u64 mut := 1;
	if(buffer_rescount > 0)
	{
		size = buffer_rescount * __sizeof(u64);
	}
	meta_create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := null;
		.flags := 0;
		.size := size;
		.usage := (0x00000020 | 0x00000002);
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
	};
	vk_check(vk.create_buffer(used_device, ref meta_create, null, ref (passptr->metabuf)));

	// create pipeline.
	passptr->is_compute = impl_shader_is_compute(info.shader);
	if(passptr->is_compute)
	{
		passptr->pipeline = impl_create_compute_pipeline(info.shader);
	}
	if(!(passptr->is_compute))
	{
		puts("graphics passes are NYI");
	}
	return retid@tz_gpu_pass;
};

== build ==
{
	add_source_file("src/vulkan.psy");
	add_source_file("src/wnd.psy");
	add_source_file("src/file.psy");

	run_command("glslc -o build/vertex.spv src/vertex.glsl");
	run_command("glslc -o build/fragment.spv src/fragment.glsl");
}
