tz_gpu_appinfo ::= struct
{
	name : u8?;
	ver_maj : u32;
	ver_min : u32;
};

tz_gpu_err ::= enum
{
	.none := 0;
	.unknown := 1;
};

tz_gpu_hardware_type ::= enum
{
	.gpu := 0;
	.igpu := 1;
	.cpu := 2;
	.unknown := 3;
};

tz_gpu_hardware_caps ::= enum
{
	.graphics_compute := 0;
	.graphics_only := 1;
	.compute_only := 2;
	.none := 3;
};

tz_gpu_hardware ::= struct
{
	name : u8 mut?;
	target_heap_gpu : u64;
	target_heap_cpu : u64;
	vram_size_mib : u64;
	type : tz_gpu_hardware_type;
	caps : tz_gpu_hardware_caps;
	id : u32;
	native_handle : u64;
};

tz_gpu_buffer_flag ::= enum
{
	.none := 0b0000;
	.dynamic := 0b0001;
	.index_buffer := 0b0010;
	.draw_buffer := 0b0100;
};

tz_gpu_image_flag ::= enum
{
	.none := 0;
};

tz_gpu_resource_flag ::= enum
{
	.none := 0;
	// ignore tz_gpu_resource_info.data, fill size with zeroes.
	.zero_memory := 0b00000001;
};

tz_gpu_image_type ::= enum
{
	.rgba := 0;
	.depth := 1;
};

tz_gpu_resource_info ::= struct
{
	data : v0? weak;
	data_size : u64;
	image_dimensions : u32[2];
	image_type : tz_gpu_image_type;
	flags : tz_gpu_resource_flag;
	buffer_flags : tz_gpu_buffer_flag;
	image_flags : tz_gpu_image_flag;
};

tz_gpu_resource ::= enum
{
	.invalid := -1;
	.window_resource := -2;
};

tz_gpu_shader_sources ::= struct
{
	vertex_spv_data : u8 mut?;
	vertex_spv_count : u64;
	fragment_spv_data : u8 mut?;
	fragment_spv_count : u64;
};

tz_gpu_shader ::= enum
{
	.invalid := -1;
};

tz_gpu_cull ::= enum
{
	.none := 0;
	.back := 2;
	.front := 1;
	.both := 3;
};

tz_gpu_graphics_state ::= struct
{
	clear_colour : f32[4];
	scissor : u32[4];
	colour_targets_data : tz_gpu_resource?;
	colour_targets_count : u64;
	depth_target : tz_gpu_resource;
	index_buffer : tz_gpu_resource;
	draw_buffer : tz_gpu_resource;
	culling : tz_gpu_cull;
	static_tri_count : u64;
};

tz_gpu_compute_state ::= struct
{
	kernelx : u32;
	kernely : u32;
	kernelz : u32;
};

tz_gpu_pass_info ::= struct
{
	graphics : tz_gpu_graphics_state;
	compute : tz_gpu_compute_state;
	shader : tz_gpu_shader;
	resources_data : tz_gpu_resource?;
	resources_count : u64;
	name : u8?;
};

tz_gpu_pass ::= enum
{
	.invalid := -1;
	.present := -2;
};

tz_gpu_graph ::= enum
{
	.invalid := -1;
};

tz_gpu_draw_command ::= struct
{
	vertex_count : u32;
	instance_count : u32;
	first_vertex : u32;
	first_instance : u32;
};

tz_gpu_draw_indexed_command ::= struct
{
	index_count : u32;
	instance_count : u32;
	first_index : u32;
	vertex_offset : s32;
	first_instance : u32;
};

valloc_initial_size : u64 static := (32 * 1024 * 1024); // 32 mib

valloc_t ::= struct
{
	device_mem : u64;
	size : u64;
	cursor : u64;
	mapped_ptr : v0? mut;
};

vallocator_t ::= struct
{
	buffer_gpu : valloc_t;
	image_gpu : valloc_t;
	buffer_cpu : valloc_t;
};

frame_data ::= struct
{
	cpool : u64;
	cmds : u64;

	swapchain_fence : u64;
	swapchain_sem : u64;
};

scratch_data ::= struct
{
	cpool : u64;
	cmds : u64;
	fence : u64;
};

// globals.
MAX_GLOBAL_IMAGE_COUNT ::= 8192;
MAX_IMAGE_COUNT_PER_PASS ::= 4096;

vallocator : vallocator_t mut;
scratch : scratch_data mut;
frame_overlap ::= 2;
frames : frame_data mut[2] mut;
set_layouts : u64 mut[2] mut;
descriptor_pools_data : u64 mut? mut;
descriptor_pools_count : u64 mut := 0;
descriptor_pools_cap : u64 mut := 0;
vkinst : u64 mut;
used_device : u64 mut;
used_hardware : u64 mut;
used_qfi : u32 mut;
used_mti_gpu : u32 mut;
used_mti_cpu : u32 mut;
graphics_queue : u64 mut;
compute_queue : u64 mut;

pipeline_layout : u64 mut;

surface : u64 mut := 0;
swapchain : u64 mut := 0;
swapchain_width : u64 mut := 0;
swapchain_height : u64 mut := 0;
swapchain_image_count : u32 mut;
swapchain_images : u64 mut? mut;
swapchain_images_cap : u64 mut := 0;
swapchain_views : u64 mut? mut;
swapchain_views_cap : u64 mut := 0;
swapchain_format ::= 44;
rgba_format ::= 37;
depth_format ::= 126;

target_window : tz_wnd mut;

system_image : u64 mut := 0;
//system_depth_image : u64 mut;
system_image_view : u64 mut := 0;

current_frame : u64 mut := 0;

resource_data_t ::= struct
{
	info : tz_gpu_resource_info;
	is_buffer : bool;
	vk_handle : u64;
	image_view : u64;
	sampler : u64;
	buffer_device_address : u64;
	mapped_ptr : v0 mut? weak;

};
resources : resource_data_t mut? mut;
resource_count : u64 mut := 0;
resource_cap : u64 mut := 0;

shader_data_t ::= struct
{
	is_graphics : bool;
	vertex_module : u64;
	fragment_module : u64;
	compute_module : u64;
};

shaders : shader_data_t mut? mut;
shaders_count : u64 mut := 0;
shaders_cap : u64 mut := 0;

graph_entry ::= struct
{
	handle : u64 mut;
	is_graph : bool mut;
};

graph_data_t ::= struct
{
	name : u8 mut?;
	timeline : graph_entry mut?;
	timeline_count : u64;
	timeline_cap : u64;
};

graphs : graph_data_t mut? mut;
graph_count : u64 mut := 0;
graph_cap : u64 mut := 0;

// internal pass data.
pass_data_t ::= struct
{
	info : tz_gpu_pass_info;
	metabuf : u64;
	metabuf_size : u64;
	is_compute : bool;
	pipeline : u64;

	targets_swapchain : bool;
	colour_target_dimensions : u32[2];

	descriptor_sets : u64 mut[2];
};

passes : pass_data_t mut? mut;
passes_count : u64 mut := 0;
passes_cap : u64 mut := 0;

// implementation details

impl_pass_uses_resource ::= func(pass : tz_gpu_pass, res : tz_gpu_resource -> bool)
{
	counter : u64 mut;
	cur_res : tz_gpu_resource mut;
	for(counter = 0, counter < passes_count, counter = counter + 1)
	{
		passptr : pass_data_t? := passes # counter;
		passinfo ::= passptr->info;
		cur_residx : u64 mut;
		for(cur_residx = 0, cur_residx < (passinfo.resources_count), cur_residx = cur_residx + 1)
		{
			cur_res = deref((passinfo.resources_data) # cur_residx);
			if(cur_res == res)
			{
				return true;
			}
		}
	}
	return false;
};

impl_resource_size_bytes ::= func(resptr : resource_data_t? -> u64)
{
	rinfo ::= resptr->info;
	return rinfo.data_size;
};

impl_new_descriptor_pool ::= func(a : arena mut? -> u64)
{
	if(descriptor_pools_cap == 0)
	{
		descriptor_pools_cap = 4;
		descriptor_pools_data = arena_alloc(a, descriptor_pools_cap * __sizeof(deref descriptor_pools_data));
	}
	if(descriptor_pools_count >= descriptor_pools_cap)
	{
		putzstr("todo: expand allocation of descriptor pool handles");
		__debugbreak();
	}

	image_limit ::= VkDescriptorPoolSize
	{
		.type := 1; // VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER
		.descriptorCount := MAX_IMAGE_COUNT_PER_PASS;
	};
	create ::= VkDescriptorPoolCreateInfo
	{
		.sType := 33;
		.pNext := zero;
		.flags := 0x00000002;
		.maxSets := frame_overlap * 256;
		.poolSizeCount := 1;
		.pPoolSizes := ref image_limit;
	};
	newpool : u64 mut;
	vk_check(vk.create_descriptor_pool(used_device, ref create, zero, ref newpool));
	deref(descriptor_pools_data # descriptor_pools_count) = newpool;
	descriptor_pools_count = descriptor_pools_count + 1;
	return newpool;
};

impl_try_allocate_descriptors ::= func(pool : u64, sets_data : u64 mut?, image_count : u32 -> bool)
{
	variable_counts : u32 mut[2];
	deref(variable_counts # 0) = image_count;
	deref(variable_counts # 1) = image_count;

	variable_alloc ::= VkDescriptorSetVariableDescriptorCountAllocateInfo
	{
		.sType := 1000161003;
		.pNext := zero;
		.descriptorSetCount := 2;
		.pDescriptorCounts := variable_counts # 0;
	};
	alloc ::= VkDescriptorSetAllocateInfo
	{
		.sType := 34;
		.pNext := (ref variable_alloc)@_;
		.descriptorPool := pool;
		.descriptorSetCount := 2;
		.pSetLayouts := set_layouts # 0;
	};
	return (vk.allocate_descriptor_sets(used_device, ref alloc, sets_data)) == 0;
};

impl_populate_descriptors ::= func(passptr : pass_data_t mut?, long : arena mut?, short : arena mut? -> v0)
{
	pinfo ::= passptr->info;
	if(descriptor_pools_count == 0)
	{
		impl_new_descriptor_pool(long);
	}
	pool : u64 mut := deref(descriptor_pools_data # (descriptor_pools_count - 1));

	counter : u64 mut;
	image_count : u32 mut := 0;
	cur_resh : tz_gpu_resource mut;
	resptr : resource_data_t? mut;
	for(counter = 0, counter < (pinfo.resources_count), counter = counter + 1)
	{
		cur_resh = deref((pinfo.resources_data) # counter);
		if(cur_resh != (tz_gpu_resource.invalid))
		{
			if(cur_resh == (tz_gpu_resource.window_resource))
			{
				image_count = image_count + 1;
			}
			if(cur_resh != (tz_gpu_resource.window_resource))
			{
				resptr = resources # (cur_resh@s64);
				if(!(resptr->is_buffer))
				{
					image_count = image_count + 1;
				}
			}
		}
	}

	// allocate descriptors
	alloc_success : bool mut := impl_try_allocate_descriptors(pool, passptr->descriptor_sets # 0, image_count);
	while(!alloc_success)
	{
		pool = impl_new_descriptor_pool(long);
		alloc_success = impl_try_allocate_descriptors(pool, passptr->descriptor_sets # 0, image_count);
	}

	// write to them.
	image_array_descriptor_binding ::= 1;
	image_writes : VkDescriptorImageInfo mut? := arena_alloc(short, __sizeof(VkDescriptorImageInfo) * image_count * frame_overlap);

	meta_buffer_write ::= VkDescriptorBufferInfo
	{
		.buffer := passptr->metabuf;
		.offset := 0;
		.range := ~0;
	};

	descriptor_writes : VkWriteDescriptorSet mut[4];
	// first write to the meta buffers.
	deref(descriptor_writes # 0) = VkWriteDescriptorSet
	{
		.sType := 35;
		.pNext := zero;
		.dstSet := deref(passptr->descriptor_sets # 0);
		.dstBinding := 0;
		.dstArrayElement := 0;
		.descriptorCount := 1;
		.descriptorType := 7; // VK_DESCRIPTOR_TYPE_STORAGE_BUFFER
		.pImageInfo := zero;
		.pBufferInfo := ref meta_buffer_write;
		.pTexelBufferView := zero;
	};

	deref(descriptor_writes # 1) = VkWriteDescriptorSet
	{
		.sType := 35;
		.pNext := zero;
		.dstSet := deref(passptr->descriptor_sets # 1);
		.dstBinding := 0;
		.dstArrayElement := 0;
		.descriptorCount := 1;
		.descriptorType := 7; // VK_DESCRIPTOR_TYPE_STORAGE_BUFFER
		.pImageInfo := zero;
		.pBufferInfo := ref meta_buffer_write;
		.pTexelBufferView := zero;
	};

	// now let's do the images.
	i : u32 mut;
	j : u64 mut;
	img_cursor : u64 mut := 0;
	for(i = 0, i < frame_overlap, i = i + 1)
	{
		for(j = 0, j < (pinfo.resources_count), j = j + 1)
		{
			cur_resh = deref((pinfo.resources_data) # j);
			if(cur_resh != (tz_gpu_resource.invalid))
			{
				if(cur_resh != (tz_gpu_resource.window_resource))
				{
					resptr = resources # (cur_resh@s64);
					if(!(resptr->is_buffer))
					{
						deref(image_writes # img_cursor) = VkDescriptorImageInfo
						{
							.sampler := resptr->sampler;
							.imageView := resptr->image_view;
							.imageLayout := 5; // VK_IMAGE_LAYOUT_SHADER_READ_ONLY_OPTIMAL
						};
						img_cursor = img_cursor + 1;
					}
				}
			}
		}
		deref(descriptor_writes # (2 + i)) = VkWriteDescriptorSet
		{
			.sType := 35;
			.pNext := zero;
			.dstSet := deref(passptr->descriptor_sets # i);
			.dstBinding := image_array_descriptor_binding;
			.dstArrayElement := 0;
			.descriptorCount := image_count;
			.descriptorType := 1; // VK_DESCRIPTOR_TYPE_COMBINED_IMAGE_SAMPLER
			.pImageInfo := image_writes # (i * image_count);
			.pBufferInfo := ref meta_buffer_write;
			.pTexelBufferView := zero;
		};
	}

	write_count : u32 mut := 4;
	if(image_count == 0)
	{
		write_count = 2;
	}
	vk.update_descriptor_sets(used_device, write_count, descriptor_writes # 0, 0, zero);
};

impl_shader_is_compute ::= func(shader : tz_gpu_shader -> bool)
{
	shadptr ::= (shaders # (shader@s64));
	return (shadptr->compute_module) != 0;
};

impl_begin_scratch_commands ::= func( -> v0)
{
	create ::= VkCommandBufferBeginInfo
	{
		.sType := 42;
		.pNext := zero;
		.flags := 0x00000001; //VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT
		.pInheritanceInfo := zero;
	};

	vk_check(vk.begin_command_buffer(scratch.cmds, ref create));
};

impl_end_and_execute_scratch_commands ::= func( -> v0)
{
	vk_check(vk.end_command_buffer(scratch.cmds));
	submit ::= VkSubmitInfo
	{
		.sType := 4;
		.pNext := zero;
		.waitSemaphoreCount := 0;
		.pWaitSemaphores := zero;
		.pWaitDstStageMask := zero;
		.commandBufferCount := 1;
		.pCommandBuffers := ref (scratch.cmds);
		.signalSemaphoreCount := 0;
		.pSignalSemaphores := zero;
	};
	vk_check(vk.queue_submit(graphics_queue, 1, ref submit, scratch.fence));
	vk_check(vk.wait_for_fences(used_device, 1, ref (scratch.fence), 1, -1@u64));
	vk_check(vk.reset_fences(used_device, 1, ref (scratch.fence)));
};

impl_create_graphics_pipeline ::= func(shader : tz_gpu_shader, graphics : tz_gpu_graphics_state, a : arena mut? -> u64)
{
	shadptr ::= (shaders # (shader@s64));
	vertex_shader_module ::= shadptr->vertex_module;
	if(vertex_shader_module == 0)
	{
		putzstr("invalid vertex shader");
		__debugbreak();
	}
	fragment_shader_module ::= shadptr->fragment_module;
	if(fragment_shader_module == 0)
	{
		putzstr("invalid fragment shader");
		__debugbreak();
	}

	shader_creates : VkPipelineShaderStageCreateInfo mut[2];
	deref(shader_creates # 0) = VkPipelineShaderStageCreateInfo
	{
		.sType := 18;
		.pNext := zero;
		.flags := 0;
		.stage := 0x00000001;
		.module := vertex_shader_module;
		.pName := "main";
		.pSpecializationInfo := zero;

	};
	deref(shader_creates # 1) = VkPipelineShaderStageCreateInfo
	{
		.sType := 18;
		.pNext := zero;
		.flags := 0;
		.stage := 0x00000010;
		.module := fragment_shader_module;
		.pName := "main";
		.pSpecializationInfo := zero;
	};

	vtx ::= VkPipelineVertexInputStateCreateInfo
	{
		.sType := 19;
		.pNext := zero;
		.flags := 0;
		.vertexBindingDescriptionCount := 0;
		.pVertexBindingDescriptions := zero;
		.vertexAttributeDescriptionCount := 0;
		.pVertexAttributeDescriptions := zero;
	};

	iasm ::= VkPipelineInputAssemblyStateCreateInfo
	{
		.sType := 20;
		.pNext := zero;
		.flags := 0;
		.topology := 3;
		.primitiveRestartEnable := 0;
	};

	tess ::= VkPipelineTessellationStateCreateInfo
	{
		.sType := 21;
		.pNext := zero;
		.flags := 0;
		.patchControlPoints := 3;
	};

	vwprt ::= VkPipelineViewportStateCreateInfo
	{
		.sType := 22;
		.pNext := zero;
		.flags := 0;
		.viewportCount := 1;
		//remember: this is dynamic state so we pass zeros even though there will be a viewport and scissor.
		.pViewports := zero;
		.scissorCount := 1;
		.pScissors := zero;
	};

	cull_bits : s32 mut;
	if(graphics.culling == (tz_gpu_cull.both))
	{
		cull_bits = 3;
	}
	if(graphics.culling == (tz_gpu_cull.front))
	{
		cull_bits = 1;
	}
	if(graphics.culling == (tz_gpu_cull.back))
	{
		cull_bits = 2;
	}
	if(graphics.culling == (tz_gpu_cull.none))
	{
		cull_bits = 0;
	}

	raster ::= VkPipelineRasterizationStateCreateInfo
	{
		.sType := 23;
		.pNext := zero;
		.flags := 0;
		.depthClampEnable := 0;
		.rasterizerDiscardEnable := 0;
		.polygonMode := 0;
		.cullMode := cull_bits;
		.frontFace := 0;
		.depthBiasEnable := 0;
		.depthBiasConstantFactor := 0.0;
		.depthBiasClamp := 0.0;
		.depthBiasSlopeFactor := 0.0;
		.lineWidth := 1.0;
	};

	multi ::= VkPipelineMultisampleStateCreateInfo
	{
		.sType := 24;
		.pNext := zero;
		.flags := 0;
		.rasterizationSamples := 1;
		.sampleShadingEnable := 0;
		.minSampleShading := 1.0;
		.pSampleMask := zero;
		.alphaToCoverageEnable := 0;
		.alphaToOneEnable := 0;
	};

	depthTestEnable : u32 mut := 0;
	depthWriteEnable : u32 mut := 1;

	stencil_op_state ::= VkStencilOpState
	{
		.failOp := 0;
		.passOp := 0;
		.depthFailOp := 0;
		.compareOp := 0;
		.compareMask := 0;
		.writeMask := 0;
		.reference := 0;
	};

	depth_stencil ::= VkPipelineDepthStencilStateCreateInfo
	{
		.sType := 25;
		.pNext := zero;
		.flags := 0;
		.depthTestEnable := depthTestEnable;
		.depthWriteEnable := depthWriteEnable;
		.depthCompareOp := 1;
		.depthBoundsTestEnable := 1;
		.stencilTestEnable := 0;
		.front := stencil_op_state;
		.back := stencil_op_state;
		.minDepthBounds := 0.0;
		.maxDepthBounds := 1.0;
	};

	counter : u64 mut;
	blend_states : VkPipelineColorBlendAttachmentState mut? := arena_alloc(a, __sizeof(VkPipelineColorBlendAttachmentState) * (graphics.colour_targets_count));
	for(counter = 0, counter < (graphics.colour_targets_count), counter = counter + 1)
	{
		deref(blend_states # counter) = VkPipelineColorBlendAttachmentState
		{
			.blendEnable := 0;
			.srcColorBlendFactor := 1;
			.dstColorBlendFactor := 0;
			.colorBlendOp := 0;
			.srcAlphaBlendFactor := 1;
			.dstAlphaBlendFactor := 0;
			.alphaBlendOp := 0;
			.colorWriteMask := (0x00000001 | 0x00000002 | 0x00000004 | 0x00000008);
		};
	}

	blend ::= VkPipelineColorBlendStateCreateInfo
	{
		.sType := 26;
		.pNext := zero;
		.flags := 0;
		.logicOpEnable := 0;
		.logicOp := 0;
		.attachmentCount := (graphics.colour_targets_count)@_;
		.pAttachments := blend_states # 0;
	};
	deref((blend.blendConstants) # 0) = 0.0;
	deref((blend.blendConstants) # 1) = 0.0;
	deref((blend.blendConstants) # 2) = 0.0;
	deref((blend.blendConstants) # 3) = 0.0;

	color_formats : s32 mut? := arena_alloc(a, __sizeof(s32) * (graphics.colour_targets_count));
	for(counter = 0, counter < (graphics.colour_targets_count), counter = counter + 1)
	{
		deref(color_formats # counter) = rgba_format;
	}
	counter = 0;

	rendering ::= VkPipelineRenderingCreateInfo
	{
		.sType := 1000044002;
		.pNext := zero;
		.viewMask := 0;
		.colorAttachmentCount := (graphics.colour_targets_count)@_;
		.pColorAttachmentFormats := color_formats;
		.depthAttachmentFormat := 126;
		.stencilAttachmentFormat := 0;
	};

	dynamic_states : s32 mut[2];
	deref(dynamic_states # 0) = 0; // VK_DYNAMIC_STATE_VIEWPORT
	deref(dynamic_states # 1) = 1; // VK_DYNAMIC_STATE_SCISSOR

	dyn ::= VkPipelineDynamicStateCreateInfo
	{
		.sType := 27;
		.pNext := zero;
		.flags := 0;
		.dynamicStateCount := (__sizeof(dynamic_states) / __sizeof(deref(dynamic_states # 0)));
		.pDynamicStates := dynamic_states # 0;
	};

	create ::= VkGraphicsPipelineCreateInfo
	{
		.sType := 28;
		.pNext := (ref rendering)@_;
		.flags := 0;
		.stageCount := 2;
		.pStages := shader_creates # 0;
		.pVertexInputState := ref vtx;
		.pInputAssemblyState := ref iasm;
		.pTessellationState := ref tess;
		.pViewportState := ref vwprt;
		.pRasterizationState := ref raster;
		.pMultisampleState := ref multi;
		.pDepthStencilState := ref depth_stencil;
		.pColorBlendState := ref blend;
		.pDynamicState := ref dyn;
		.layout := pipeline_layout;
		.renderPass := 0;
		.subpass := 0;
		.basePipelineHandle := 0;
		.basePipelineIndex := -1;
	};

	ret : u64 mut;
	vk_check(vk.create_graphics_pipelines(used_device, 0, 1, ref create, zero, ref ret));
	return ret;
};

impl_create_compute_pipeline ::= func(shader : tz_gpu_shader, compute : tz_gpu_compute_state -> u64)
{
	shadptr ::= (shaders # (shader@s64));
	compute_shader_module ::= shadptr->compute_module;
	if(compute_shader_module == 0)
	{
		putzstr("invalid compute shader");
		__debugbreak();
	}
	create ::= VkComputePipelineCreateInfo
	{
		.sType := 29;
		.pNext := zero;
		.flags := 0;
		.stage := VkPipelineShaderStageCreateInfo
		{
			.sType := 18;
			.pNext := zero;
			.flags := 0;
			.stage := 0x00000020; 
			.module := compute_shader_module;
			.pName := "main";
			.pSpecializationInfo := zero;
		};
		.layout := pipeline_layout;
		.basePipelineHandle := 0;
		.basePipelineIndex := -1;
	};
	ret : u64 mut;

	vk_check(vk.create_compute_pipelines(used_device, 0, 1, ref create, zero, ref ret));
	return ret;
};

impl_bind_image_mem ::= func(image : u64, imgsize : u64 -> v0)
{
	valloc : valloc_t mut? mut := ref (vallocator.image_gpu);
	if((valloc->cursor + imgsize) > (valloc->size))
	{
		putzstr("allocation too large. requested ");
		putuint(valloc->cursor + imgsize);
		putzstr("B but there was only ");
		putuint(valloc->size);
		putzstr("B remaining.");
		__debugbreak();
	}

	memreqs : VkMemoryRequirements mut;
	vk.get_image_memory_requirements(used_device, image, ref memreqs);

	padding ::= ((memreqs.align) - ((valloc->cursor) % (memreqs.align)) % (memreqs.align));
	(valloc->cursor) = (valloc->cursor) + (padding);

	ret ::= vk.bind_image_memory(used_device, image, valloc->device_mem, valloc->cursor);
	if(ret == -1) // out of host memory 
	{
		putzstr("oom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) // out of device memory 
	{
		putzstr("voom when binding image memory. todo: multiple allocations");
		__debugbreak();
	}
	// todo: align safely.
	valloc->cursor = (valloc->cursor) + imgsize;
};

impl_need_swapchain ::= func(w : u64, h : u64, a : arena mut? -> bool)
{
	if(target_window == (tz_wnd.invalid))
	{
		putzstr("one or more GPU passes need access to the swapchain, but no valid window was ever provided via tz_gpu_use_hardware.");
		__debugbreak();
	}
	if(surface == 0)
	{
		// create surface.
		static if(_win32)
		{
			create ::= VkWin32SurfaceCreateInfoKHR
			{
				.sType := 1000009000;
				.pNext := zero;
				.flags := 0;
				.hinstance := GetModuleHandleA(zero);
				.hwnd := tz_window_native(target_window);
			};
			vk_check(vk.create_win32_surface_khr(vkinst, ref create, zero, ref surface));
		}
		static if(_linux)
		{
			__error("linux vulkan support NYI");
		}
	}

	if(swapchain_width@_ == w)
	{
		if(swapchain_height@_ == h)
		{
			return false;
		}
	}

	putzstr("(re)creating swapchain...");
	putchar(10);

	caps : VkSurfaceCapabilitiesKHR mut;
	vk.get_physical_device_surface_capabilities_khr(used_hardware, surface, ref caps);

	swch_create ::= VkSwapchainCreateInfoKHR
	{
		.sType := 1000001000;
		.pNext := zero;
		.flags := 0;
		.surface := surface;
		.minImageCount := 2;
		.imageFormat := swapchain_format;
		.imageColorSpace := 0;
		.imageExtent := VkExtent2D{.width := w@_; .height := h@_;};
		.imageArrayLayers := 1;
		.imageUsage := (0x00000010 | 0x00000002);
		.imageSharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
		.preTransform := caps.currentTransform;
		.compositeAlpha := 0x00000001;
		// 0 = immediate. fifo (vsync) = 2
		.presentMode := 0;
		.clipped := 0;
		.oldSwapchain := swapchain;
	};
	old_swapchain ::= swapchain;
	vk_check(vk.create_swapchain_khr(used_device, ref swch_create, zero, ref swapchain));

	// todo: destroy old swapchain images if they already existed. right now we arent checking that and are just stomping over swapchain_images and swapchain_views.
	vk.get_swapchain_images_khr(used_device, swapchain, ref swapchain_image_count, zero);
	if(swapchain_images_cap != (swapchain_image_count@_))
	{
		swapchain_images = arena_alloc(a, __sizeof(u64) * swapchain_image_count);
		swapchain_images_cap = (swapchain_image_count@_);
	}
	if(swapchain_views_cap != (swapchain_image_count@_))
	{
		swapchain_views = arena_alloc(a, __sizeof(u64) * swapchain_image_count);
		swapchain_views_cap = (swapchain_image_count@_);
	}
	vk.get_swapchain_images_khr(used_device, swapchain, ref swapchain_image_count, swapchain_images);

	counter : u32 mut;
	if(old_swapchain != 0)
	{
		// we already had a swapchain. we should delete the old images.
		for(counter = 0, counter < swapchain_image_count, counter = counter + 1)
		{
			vk.destroy_image_view(used_device, deref(swapchain_views # counter), zero);
		}
		vk.destroy_swapchain_khr(used_device, old_swapchain, zero);
	}

	view_create : VkImageViewCreateInfo mut := VkImageViewCreateInfo
	{
.sType := 15;
		.pNext := zero;
		.flags := 0;
		.image := 0; // note: will overwrite this in coming for-loop.
		.viewType := 1;
		.format := swapchain_format;
		.components := VkComponentMapping
		{
			.r := 0;
			.g := 0;
			.b := 0;
			.a := 0;
		};
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := 0x00000001;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};

	for(counter = 0, counter < swapchain_image_count, counter = counter + 1)
	{
		(view_create.image) = deref(swapchain_images # counter);

		vk_check(vk.create_image_view(used_device, ref view_create, zero, swapchain_views # counter));
	}

	if(system_image != 0)
	{
		// destroy old system image + view if they already exist. i am not bothering rn.
		vk.destroy_image_view(used_device, system_image_view, zero);
		vk.destroy_image(used_device, system_image, zero);
	}
	system_image_create ::= VkImageCreateInfo
	{
		.sType := 14;
		.pNext := zero;
		.flags := 0;
		.imageType := 1;
		.format := rgba_format;
		.extent := VkExtent3D
		{
			.width := w@_;
			.height := h@_;
			.depth := 1;
		};
		.mipLevels := 1;
		.arrayLayers := 1;
		.samples := 1;
		.tiling := 0;
		.usage := (0x00000010) | (0x00000001); // VK_IMAGE_USAGE_COLOR_ATTACHMENT_BIT | VK_IMAGE_USAGE_TRANSFER_SRC_BIT
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
		.initialLayout := 0;
	};
	vk_check(vk.create_image(used_device, ref system_image_create, zero, ref system_image));
	impl_bind_image_mem(system_image, w * h * 4);
	view_create.image = system_image;
	view_create.format = rgba_format;
	vk_check(vk.create_image_view(used_device, ref view_create, zero, ref system_image_view));

	swapchain_width = w;
	swapchain_height = h;
	// todo: system depth image

	return true;
};

impl_require_swapchain ::= func(a : arena mut? -> bool)
{
	windims ::= tz_window_dimensions(target_window);
	x ::= deref(windims # 0);
	y ::= deref(windims # 1);
	if(x == 0)
	{
		swapchain_width = 0;
		swapchain_height = 0;
	}
	if(y == 0)
	{
		swapchain_width = 0;
		swapchain_height = 0;
	}
	return impl_need_swapchain(x@_, y@_, a);
};

impl_get_pass_colour_target_dimensions ::= func(passptr : pass_data_t? -> u32[2])
{
	pinfo ::= passptr->info;
	ginfo ::= pinfo.graphics;
	if(ginfo.colour_targets_count == 0)
	{
		putzstr("bad pass - no colour targets.");
	}
	first_colour_target : tz_gpu_resource mut := zero;
	first_colour_target = deref ((ginfo.colour_targets_data) # 0);
	//first_colour_target ::= deref ((ginfo.colour_targets_data) # 0);
	if(first_colour_target == (tz_gpu_resource.window_resource))
	{
		return u32[2]
		{
			swapchain_width@_;
			swapchain_height@_;
		};
	}
	res ::= (resources # (first_colour_target@s64));
	resinfo ::= res->info;
	return resinfo.image_dimensions;
};

impl_record_compute_work ::= func(passptr : pass_data_t mut?, frame_id : u64 -> v0)
{
	putzstr("compute work is NYI");
	__debugbreak();
};

impl_record_graphics_work ::= func(passptr : pass_data_t mut?, frame_id : u64, a : arena mut? -> v0)
{
	frame ::= deref(frames # frame_id);
	pinfo ::= passptr->info;
	ginfo ::= pinfo.graphics;
	ccount ::= ginfo.colour_targets_count;

	if(ccount == 0)
	{
		putzstr("doesn't make sense to have a graphics pass with no colour attachments. please use one.");
		__debugbreak();
	}

	colour_attachments : VkRenderingAttachmentInfo mut? := arena_alloc(a, __sizeof(VkRenderingAttachmentInfo) * ccount);
	colour_transitions : VkImageMemoryBarrier mut? := arena_alloc(a, __sizeof(VkImageMemoryBarrier) * ccount);
	colour_transition_count : u64 mut := 0;

	counter : u64 mut := 0;
	colour_target : tz_gpu_resource mut;
	resptr : resource_data_t mut? mut;
	render_target : u64 mut;
	render_target_view : u64 mut;

	if(passptr->targets_swapchain)
	{
		impl_require_swapchain(a);

		dims ::= passptr->colour_target_dimensions;
		if(deref(dims # 0) != swapchain_width@_)
		{
			(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
		}
		if(deref(dims # 1) != swapchain_height@_)
		{
			(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
		}
	}
	dimensions ::= (passptr->colour_target_dimensions);
	w ::= deref(dimensions # 0);
	h ::= deref(dimensions # 1);

	for(counter = 0, counter < ccount, counter = counter + 1)
	{
		colour_target = deref ((ginfo.colour_targets_data) # counter);
		render_target = 0;
		render_target_view = 0;

		if(colour_target == (tz_gpu_resource.window_resource))
		{
			// need a new swapchain!
			render_target = system_image;
			render_target_view = system_image_view;
		}
		//else
		if(colour_target != (tz_gpu_resource.window_resource))
		{
			resptr = (resources # (colour_target@s64));
			render_target = (resptr->vk_handle);
			render_target_view = (resptr->image_view);
		}

		deref(colour_attachments # counter) = VkRenderingAttachmentInfo
		{
			.sType := 1000044001;
			.pNext := zero;
			.imageView := render_target_view;
			.imageLayout := 2; //VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
			.resolveMode := 0;
			.resolveImageView := 0;
			.resolveImageLayout := 0;
			//.loadOp := 0; // VK_ATTACHMENT_LOAD_OP_LOAD (if we're noit clearing colour targets)
			.loadOp := 1; // VK_ATTACHMENT_LOAD_OP_CLEAR (if we are clearing colour targets)
			.storeOp := 0;
			.clearValue := VkClearValue{.color := VkClearColorValue{.float32 := ginfo.clear_colour;};};
		};
		if(true) //todo: if we're not clearing colour targets
		{
			deref (colour_transitions # colour_transition_count) = VkImageMemoryBarrier
			{
				.sType := 45;
				.pNext := zero;
				.srcAccessMask := 0;
				.dstAccessMask := 0x00000100; // VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT
				.oldLayout := 0;
				.newLayout := 2; // VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
				.srcQueueFamilyIndex := ~0;
				.dstQueueFamilyIndex := ~0;
				.image := render_target;
				.subresourceRange := VkImageSubresourceRange
				{
					.aspectMask := 0x00000001; // VK_IMAGE_ASPECT_COLOR_BIT 
					.baseMipLevel := 0;
					.levelCount := 1;
					.baseArrayLayer := 0;
					.layerCount := 1;
				};
			};
			colour_transition_count = colour_transition_count + 1;
		}
	}

	// if we have any colour transitions # all, do a pipeline barrier with them.
	if(colour_transition_count > 0)
	{
		vk.cmd_pipeline_barrier(frame.cmds, 0x00002000, 0x00000400, 0, 0, zero, 0, zero, colour_transition_count@_, colour_transitions);
	}
	// todo: depth target stuffs.
	render ::= VkRenderingInfo
	{
		.sType := 1000044000;
		.pNext := zero;
		.flags := 0;
		.renderArea := VkRect2D
		{
			.offset := VkOffset2D{.x := 0; .y := 0;};
			.extent := VkExtent2D
			{
				.width := w;
				.height := h;
			};
		};
		.layerCount := 1;
		.viewMask := 0;
		.colorAttachmentCount := ccount@_;
		.pColorAttachments := colour_attachments;
		.pDepthAttachment := zero;
		.pStencilAttachment := zero;
	};
	vk.cmd_begin_rendering(frame.cmds, ref render);
	// actually do rendering.
	vk.cmd_bind_pipeline(frame.cmds, 0, passptr->pipeline);
	// todo: bind index buffer if it exists.
	vk.cmd_bind_descriptor_sets(frame.cmds, 0, pipeline_layout, 0, 1, passptr->descriptor_sets # current_frame, 0, zero);
	viewport ::= VkViewport
	{
		.x := 0.0;
		.y := h@_;
		.width := w@_;
		.height := -1.0 * h@_;
		.minDepth := 0.0;
		.maxDepth := 1.0;
	};
	vk.cmd_set_viewport(frame.cmds, 0, 1, ref viewport);

	scissor ::= VkRect2D
	{
		.offset := VkOffset2D
		{
			.x := 0;
			.y := 0;
		};
		.extent := VkExtent2D
		{
			.width := w;
			.height := h;
		};
	};
	vk.cmd_set_scissor(frame.cmds, 0, 1, ref scissor);

	drawbufres : resource_data_t? mut;
	drawbufinfo : tz_gpu_resource_info mut;
	// todo: draw buffer logic.
	// for now we just do the draw.
	tri_count ::= ginfo.static_tri_count;
	drawbuf ::= ginfo.draw_buffer;
	idxbuf ::= ginfo.index_buffer;
	if(idxbuf == (tz_gpu_resource.invalid))
	{
		if(drawbuf == (tz_gpu_resource.invalid))
		{
			vk.cmd_draw(frame.cmds, (tri_count * 3)@_, 1, 0, 0);
		}
		if(drawbuf != (tz_gpu_resource.invalid))
		{
			drawbufres = resources # (drawbuf@s64);
			drawbufinfo = drawbufres->info;
			draw_buf_max_size_unindexed ::= ((drawbufinfo.data_size) - __sizeof(u32)) / __sizeof(tz_gpu_draw_command);
			vk.cmd_draw_indirect_count(frame.cmds, drawbufres->vk_handle, __sizeof(u32), drawbufres->vk_handle, 0, draw_buf_max_size_unindexed@_, __sizeof(tz_gpu_draw_command));
		}
	}
	if(idxbuf != (tz_gpu_resource.invalid))
	{
		idxbufres ::= resources # (idxbuf@s64);
		idxbufinfo ::= idxbufres->info;
		vk.cmd_bind_index_buffer(frame.cmds, idxbufres->vk_handle, 0, 1);
		if(drawbuf == (tz_gpu_resource.invalid))
		{
			vk.cmd_draw_indexed(frame.cmds, (tri_count * 3)@_, 1, 0, 0, 0);
		}
		if(drawbuf != (tz_gpu_resource.invalid))
		{
			drawbufres = resources # (drawbuf@s64);
			drawbufinfo = drawbufres->info;
			draw_buf_max_size_indexed ::= ((drawbufinfo.data_size) - __sizeof(u32)) / __sizeof(tz_gpu_draw_indexed_command);
			vk.cmd_draw_indexed_indirect_count(frame.cmds, drawbufres->vk_handle, __sizeof(u32), drawbufres->vk_handle, 0, draw_buf_max_size_indexed@_, __sizeof(tz_gpu_draw_indexed_command));
		}
	}

	vk.cmd_end_rendering(frame.cmds);
};

impl_record_gpu_work ::= func(pass : tz_gpu_pass, frame_id : u64, a : arena mut? -> v0)
{
	if(pass == (tz_gpu_pass.present))
	{
		return;
	}
	passptr : pass_data_t mut? := passes # (pass@s64);
	if(passptr->is_compute)
	{
		impl_record_compute_work(passptr, frame_id);
	}
	if(!(passptr->is_compute))
	{
		impl_record_graphics_work(passptr, frame_id, a);
	}
};

impl_graph_will_present ::= func(graph : tz_gpu_graph -> bool)
{
	graphptr ::= graphs # (graph@s64);
	count ::= graphptr->timeline_count;

	counter : u64 mut;
	cur_entry : graph_entry mut;
	for(counter = 0, counter < count, counter = counter + 1)
	{
		cur_entry = deref (graphptr->timeline # counter);
		if((cur_entry.handle) == ((tz_gpu_pass.present)@s64@_))
		{
			return true;
		}
	}
	return false;
};

impl_pass_writes_to_system_image ::= func(pass : tz_gpu_pass -> bool)
{
	if(pass == (tz_gpu_pass.present))
	{
		return false;
	}
	passptr ::= (passes # (pass@s64));
	passinfo ::= passptr->info;
	ginfo ::= passinfo.graphics;
	counter : u64 mut := 0;
	cur_colour_target : tz_gpu_resource mut;
	if(!(passptr->is_compute))
	{
		for(counter = 0, counter < (ginfo.colour_targets_count), counter = counter + 1)
		{
			cur_colour_target = deref((ginfo.colour_targets_data) # counter);
			if(cur_colour_target == (tz_gpu_resource.window_resource))
			{
				return true;
			}
		}
	}
	return false;
};

impl_graph_writes_to_system_image ::= func(graph : tz_gpu_graph -> bool)
{
	graphptr ::= graphs # (graph@s64);
	count ::= graphptr->timeline_count;

	counter : u64 mut;
	cur_entry : graph_entry mut;
	cur_pass : tz_gpu_pass mut;
	for(counter = 0, counter < count, counter = counter + 1)
	{
		cur_entry = deref (graphptr->timeline # counter);
		if(!(cur_entry.is_graph))
		{
			cur_pass = ((cur_entry.handle)@s64@tz_gpu_pass);
			if(impl_pass_writes_to_system_image(cur_pass))
			{
				return true;
			}
		}
	}
	return false;
};

impl_alloc_new_pass ::= func(a : arena mut? -> pass_data_t mut?)
{
	if(passes_cap == 0)
	{
		passes = arena_alloc(a, __sizeof(pass_data_t) * 32);
		passes_cap = 32;
	}
	if(passes_count > passes_cap)
	{
		putzstr("ran out of pass capacity. todo: fix this.");
		__debugbreak();
	}
	id ::= passes_count;
	passes_count = passes_count + 1;
	return passes # id;
};

impl_alloc_new_shader ::= func(a : arena mut? -> shader_data_t mut?)
{
	if(shaders_cap == 0)
	{
		shaders = arena_alloc(a, __sizeof(shader_data_t) * 32);
		shaders_cap = 32;
	}
	if(shaders_count > shaders_cap)
	{
		putzstr("ran out of shader capacity. todo: fix this.");
		__debugbreak();
	}
	id ::= shaders_count;
	shaders_count = shaders_count + 1;
	return shaders # id;
};

impl_alloc_new_resource ::= func(a : arena mut? -> resource_data_t mut?)
{
	if(resource_cap == 0)
	{
		resources = arena_alloc(a, __sizeof(resource_data_t) * 1024);
		resource_cap = 1024;
	}
	if(resource_count > resource_cap)
	{
		putzstr("ran out of resources. todo: fix this.");
		__debugbreak();
	}
	id ::= resource_count;
	resource_count = resource_count + 1;
	return resources # id;
};

impl_alloc_new_graph ::= func(a : arena mut? -> graph_data_t mut?)
{
	if(graph_cap == 0)
	{
		graphs = arena_alloc(a, __sizeof(graph_data_t) * 64);
		graph_cap = 64;
	}
	if(graph_count > graph_cap)
	{
		putzstr("ran out of graphs. todo: fix this.");
		__debugbreak();
	}
	id ::= graph_count;
	graph_count = graph_count + 1;
	return graphs # id;
};

impl_vallocator_initial_setup ::= func( -> v0)
{
	bda_flags ::= VkMemoryAllocateFlagsInfo
	{
		.sType := 1000060000;
		.pNext := zero;
		.flags := 0x00000002;
		.deviceMask := 0;
	};
	gpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := used_mti_gpu;
	};
	cpucreate ::= VkMemoryAllocateInfo
	{
		.sType := 5;
		.pNext := (ref bda_flags)@_;
		.allocationSize := valloc_initial_size;
		.memoryTypeIndex := used_mti_cpu;
	};

	// create initial allocations for each case (buffer gpu, image gpu, buffer cpu).
	bgpu ::= ref(vallocator.buffer_gpu);
	vk_check(vk.allocate_memory(used_device, ref gpucreate, zero, ref (bgpu->device_mem)));
	bgpu->size = valloc_initial_size;
	bgpu->cursor = 0;
	igpu ::= ref(vallocator.image_gpu);
	vk_check(vk.allocate_memory(used_device, ref gpucreate, zero, ref (igpu->device_mem)));
	igpu->size = valloc_initial_size;
	igpu->cursor = 0;

	bcpu ::= ref(vallocator.buffer_cpu);
	vk_check(vk.allocate_memory(used_device, ref cpucreate, zero, ref (bcpu->device_mem)));
	vk_check(vk.map_memory(used_device, bcpu->device_mem, 0, valloc_initial_size, 0, ref (bcpu->mapped_ptr)));
	
	bcpu->size = valloc_initial_size;
	bcpu->cursor = 0;
};

impl_vallocator_free_all ::= func( -> v0)
{
	bgpu ::= vallocator.buffer_gpu;
	vk.free_memory(used_device, bgpu.device_mem, zero);
	igpu ::= vallocator.image_gpu;
	vk.free_memory(used_device, igpu.device_mem, zero);

	bcpu ::= vallocator.buffer_cpu;
	vk.free_memory(used_device, bcpu.device_mem, zero);
};

impl_bind_buffer_mem ::= func(buffer : u64, bufsize : u64, gpu_memory : bool -> u64)
{
	valloc : valloc_t mut? mut := ref (vallocator.buffer_cpu);
	if(gpu_memory)
	{
		valloc = ref (vallocator.buffer_gpu);
	}
	if((valloc->cursor + bufsize) > (valloc->size))
	{
		putzstr("allocation too large. requested ");
		putuint(valloc->cursor + bufsize);
		putzstr("B but there was only ");
		putuint(valloc->size);
		putzstr("B remaining.");
		__debugbreak();
	}

	memreqs : VkMemoryRequirements mut;
	vk.get_buffer_memory_requirements(used_device, buffer, ref memreqs);
	padding ::= ((memreqs.align) - ((valloc->cursor) % (memreqs.align)) % (memreqs.align));
	(valloc->cursor) = (valloc->cursor) + (padding);
	offset_from_zero ::= valloc->cursor;

	ret ::= vk.bind_buffer_memory(used_device, buffer, valloc->device_mem, valloc->cursor);
	if(ret == -1) // out of host memory 
	{
		putzstr("oom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	if(ret == -2) // out of device memory 
	{
		putzstr("voom when binding buffer memory. todo: multiple allocations");
		__debugbreak();
	}
	(valloc->cursor) = (valloc->cursor) + bufsize;
	return offset_from_zero;
};

impl_cpu_gpu_transfer ::= func(dst_gpu_res : u64, dst_gpu_is_buffer : bool, src_data : u8? weak, src_data_size : u64, image_dimensions : u32[2] -> v0)
{
	impl_begin_scratch_commands();
	staging_create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := src_data_size;
		.usage := 0x00000001; //VK_BUFFER_USAGE_TRANSFER_SRC_BIT
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
	};
	staging : u64 mut;
	vk.create_buffer(used_device, ref staging_create, zero, ref staging);
	// todo: write to the buffer using map memory.
	old_cpumem ::= vallocator.buffer_cpu;
	cursor ::= impl_bind_buffer_mem(staging, src_data_size, false);
	cpumem ::= vallocator.buffer_cpu;
	mapped_ptr ::= (cpumem.mapped_ptr)@u8 mut?;
	memcopy(mapped_ptr # cursor, src_data, src_data_size);

	aspect_mask : s32 := 1; // todo: suport depth.

	if(dst_gpu_is_buffer)
	{
		bufcpy ::= VkBufferCopy
		{
			.srcOffset := 0;
			.dstOffset := 0;
			.size := src_data_size;
		};
		vk.cmd_copy_buffer(scratch.cmds, staging, dst_gpu_res, 1, ref bufcpy);
	}
	if(!dst_gpu_is_buffer)
	{
		imgcpy ::= VkBufferImageCopy
		{
			.bufferOffset := 0;
			.bufferRowLength := 0;
			.bufferImageHeight := 0;
			.imageSubresource := VkImageSubresourceLayers
			{
				.aspectMask := aspect_mask; // todo: do depth if the image is a depth image.
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.imageOffset := VkOffset3D{.x := 0; .y := 0; .z := 0;};
			.imageExtent := VkExtent3D{.width := deref(image_dimensions # 0); .height := deref(image_dimensions # 1); .depth := 1;};
		};
		barrier ::= VkImageMemoryBarrier
		{
			.sType := 45;
			.pNext := zero;
			.srcAccessMask := 0;
			.dstAccessMask := 0x00001000;//VK_ACCESS_TRANSFER_WRITE_BIT
			.oldLayout := 0;
			.newLayout := 7; // VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL
			.srcQueueFamilyIndex := ~0;
			.dstQueueFamilyIndex := ~0;
			.image := dst_gpu_res;
			.subresourceRange := VkImageSubresourceRange
			{
				.aspectMask := aspect_mask;
				.baseMipLevel := 0;
				.levelCount := 1;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
		};
		vk.cmd_pipeline_barrier(scratch.cmds, 0, 0x00001000, 0, 0, zero, 0, zero, 1, ref barrier);
		vk.cmd_copy_buffer_to_image(scratch.cmds, staging, dst_gpu_res, 7, 1, ref imgcpy);
	}

	impl_end_and_execute_scratch_commands();
	vk.destroy_buffer(used_device, staging, zero);
};

impl_write_a_resource ::= func(resptr : resource_data_t? -> v0)
{
	rinfo ::= resptr->info;
	is_dynamic_buffer : bool mut := resptr->is_buffer;
	if(is_dynamic_buffer)
	{
		is_dynamic_buffer = (((rinfo.buffer_flags) & (tz_gpu_buffer_flag.dynamic)) != zero);
	}
	if(is_dynamic_buffer)
	{
		memcopy(resptr->mapped_ptr, rinfo.data, rinfo.data_size);
	}
	if(!is_dynamic_buffer)
	{
		impl_cpu_gpu_transfer(resptr->vk_handle, resptr->is_buffer, rinfo.data, rinfo.data_size, rinfo.image_dimensions);
	}
};

impl_write_resources ::= func(passptr : pass_data_t mut?, long : arena mut?, short : arena mut? -> v0)
{
	pinfo ::= passptr->info;
	if((pinfo.resources_count) == 0)
	{
		return;
	}

	buffer_addresses : u64 mut? := arena_alloc(short, passptr->metabuf_size);
	bufcount : u64 mut := 0;

	i : u64 mut := 0;
	curres : tz_gpu_resource mut;
	resdata : resource_data_t mut;
	for(i = 0, i < (pinfo.resources_count), i = i + 1)
	{
		curres = deref((pinfo.resources_data) # i);
		if(curres != (tz_gpu_resource.invalid))
		{
			if(curres != (tz_gpu_resource.window_resource))
			{
				resdata = deref (resources # (curres@s64));
				if(resdata.is_buffer)
				{
					defer bufcount = bufcount + 1;
					deref(buffer_addresses # bufcount) = (resdata.buffer_device_address);
					// todo: resource write buffer data to resdata.vk_handle.
				}
				if(!(resdata.is_buffer))
				{
					// todo: resource write image data to resdata.vk_handle.
				}
			}
		}
	}

	if(bufcount != ((passptr->metabuf_size) / __sizeof(u64)))
	{
		putzstr("internal metabuf logic error");
		__debugbreak();
	}

	// write to metabuffer.
	if(bufcount > 0)
	{
		impl_cpu_gpu_transfer(passptr->metabuf, true, buffer_addresses, passptr->metabuf_size, zero);
	}
};


impl_initialise_pipeline_layout ::= func( -> u64)
{
	bindings : VkDescriptorSetLayoutBinding mut[2];
	deref(bindings # 0) = VkDescriptorSetLayoutBinding
	{
		.binding := 0;
		.descriptorType := 7; //storage buffer
		.descriptorCount := 1;
		.stageFlags := 0x7FFFFFFF;
		.pImmutableSamplers := zero;
	};
	deref(bindings # 1) = VkDescriptorSetLayoutBinding
	{
		.binding := 1;
		.descriptorType := 1; //combined image sampler
		.descriptorCount := MAX_GLOBAL_IMAGE_COUNT@u32;
		.stageFlags := 0x7FFFFFFF;
		.pImmutableSamplers := zero;
	};
	bindings_size : u32 := __sizeof(bindings) / __sizeof(deref(bindings # 0));

	flags : s32 mut[2];
	deref(flags # 0) = 0;
	deref(flags # 1) =
		(0x00000004 | //VK_DESCRIPTOR_BINDING_PARTIALLY_BOUND_BIT 
		0x00000001 | //VK_DESCRIPTOR_BINDING_UPDATE_AFTER_BIND_BIT
		0x00000002 | //VK_DESCRIPTOR_BINDING_UPDATE_UNUSED_WHILE_PENDING_BIT
		0x00000008); //VK_DESCRIPTOR_BINDING_VARIABLE_DESCRIPTOR_COUNT_BIT
	flags_size : u32 := __sizeof(flags) / __sizeof(deref(flags # 0));

	flags_create ::= VkDescriptorSetLayoutBindingFlagsCreateInfo
	{
		.sType := 1000161000;
		.pNext := zero;
		.bindingCount := flags_size@u32;
		.pBindingFlags := flags # 0;
	};

	layout_create ::= VkDescriptorSetLayoutCreateInfo
	{
		.sType := 32;
		.pNext := (ref flags_create)@_;
		.flags := 0x00000002; //VK_DESCRIPTOR_SET_LAYOUT_CREATE_UPDATE_AFTER_BIND_POOL_BIT 
		.bindingCount := bindings_size@u32;
		.pBindings := bindings # 0;
	};

	counter : u64 mut;
	for(counter = 0, counter < frame_overlap, counter = counter + 1)
	{
		vk_check(vk.create_descriptor_set_layout(used_device, ref layout_create, zero, set_layouts # counter));
	}

	create ::= VkPipelineLayoutCreateInfo
	{
		.sType := 30;
		.pNext := zero;
		.flags := 0;
		.setLayoutCount := frame_overlap@u32;
		.pSetLayouts := set_layouts # 0;
		.pushConstantRangeCount := 0;
		.pPushConstantRanges := zero;
	};

	ret : u64 mut;
	vk_check(vk.create_pipeline_layout(used_device, ref create, zero, ref ret));

	return ret;
};

impl_fill_hardware_info ::= func(hw : tz_gpu_hardware mut?, pdev : u64, a : arena mut? -> v0)
{
	props : VkPhysicalDeviceProperties mut;
	vk.get_physical_device_properties(pdev, ref props);
	memprops : VkPhysicalDeviceMemoryProperties mut;
	vk.get_physical_device_memory_properties(pdev, ref memprops);

	(hw->id) = -1;
	(hw->native_handle) = pdev;
	(hw->vram_size_mib) = 0;
	(hw->target_heap_gpu) = 0;
	(hw->target_heap_cpu) = 0;

	(hw->type) = (tz_gpu_hardware_type.unknown);
	if(props.deviceType == 0x01)
	{
		(hw->type) = (tz_gpu_hardware_type.igpu);
	}
	if(props.deviceType == 0x02)
	{
		(hw->type) = (tz_gpu_hardware_type.gpu);
	}
	if(props.deviceType == 0x04)
	{
		(hw->type) = (tz_gpu_hardware_type.cpu);
	}
	counter : u32 mut := 0;

	cur_vk_heap : VkMemoryHeap mut;
	cur_mem_ty : VkMemoryType mut;

	size_gpu : u64 mut := 0;
	size_cpu : u64 mut := 0;

	for(counter = 0, counter < (memprops.memoryTypeCount), counter = counter + 1)
	{
		cur_mem_ty = deref(memprops.memoryTypes # counter);
		cur_vk_heap = deref(memprops.memoryHeaps # (cur_mem_ty.heapIndex));
		if(((cur_mem_ty.propertyFlags) & 0x00000002) != zero) //HOST_VISIBLE
		{
			if(size_cpu < (cur_vk_heap.size))
			{
				(hw->target_heap_cpu) = counter@_;
				size_cpu = (cur_vk_heap.size);
			}
		}
		if(((cur_mem_ty.propertyFlags) & 0x00000001) != zero) //DEVICE_LOCAL
		{
			if(size_gpu < (cur_vk_heap.size))
			{
				(hw->target_heap_gpu) = counter@_;
				size_gpu = (cur_vk_heap.size);
			}
		}
	}
	counter = 0;
	for(counter = 0, counter < (memprops.memoryHeapCount), counter = counter + 1)
	{
		cur_vk_heap = deref(memprops.memoryHeaps # counter);
		(hw->vram_size_mib) = (hw->vram_size_mib) + (cur_vk_heap.size);
	}

	family_count : u32 mut := 0;
	vk.get_physical_device_queue_family_properties(pdev, ref family_count, zero);
	families : VkQueueFamilyProperties mut? := arena_alloc(a, __sizeof(VkQueueFamilyProperties) * family_count);
	vk.get_physical_device_queue_family_properties(pdev, ref family_count, families);

	cur_fam : VkQueueFamilyProperties mut;
	for(counter = 0, counter < family_count, counter = counter + 1)
	{
		// hw->idx should be set to the first graphics compute queue.
		cur_fam = deref(families # counter);
		if((cur_fam.queueFlags) & 0x03)
		{
			(hw->id) = counter;
			counter = family_count;
		}
	}

	namelen ::= zstrlen(props.deviceName # 0);
	hw->name = arena_alloc(a, namelen + 1);
	memcopy(hw->name, props.deviceName # 0, namelen);
	deref(hw->name # namelen) = 0;
};

impl_init_buffer ::= func(resptr : resource_data_t mut?, info : tz_gpu_resource_info, a : arena mut? -> v0)
{
	resptr->info = info;
	resptr->is_buffer = true;
	create : VkBufferCreateInfo mut := VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := info.data_size;
		.usage := (0x00000020 | 0x00020000);
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
	};
	dynamic ::= ((info.buffer_flags) & (tz_gpu_buffer_flag.dynamic)) != zero;
	if(!dynamic)
	{
		create.usage = (create.usage) | 0x00000002;
	}
	if(((info.buffer_flags) & (tz_gpu_buffer_flag.index_buffer)) != zero)
	{
		create.usage = (create.usage) | 0x00000040;
	}
	if(((info.buffer_flags) & (tz_gpu_buffer_flag.draw_buffer)) != zero)
	{
		create.usage = (create.usage) | 0x00000100;
	}
	vk_check(vk.create_buffer(used_device, ref create, zero, ref (resptr->vk_handle)));
	bda ::= VkBufferDeviceAddressInfo
	{
		.sType := 1000244001;
		.pNext := zero;
		.buffer := resptr->vk_handle;
	};
	cursor ::= impl_bind_buffer_mem(resptr->vk_handle, info.data_size, dynamic == false);
	if(dynamic)
	{
		cpumem ::= vallocator.buffer_cpu;
		mapped_ptr ::= (cpumem.mapped_ptr)@u8 mut?;
		(resptr->mapped_ptr) = (mapped_ptr # cursor)@_;
	}
	tmp_bda ::= vk.get_buffer_device_address(used_device, ref bda);
	(resptr->buffer_device_address) = tmp_bda;

	// make a copy of the resource data using the arena.
	resinfo ::= ref(resptr->info);
	resinfo->data = arena_alloc(a, info.data_size);
	if(info.data != zero)
	{
		memcopy(resinfo->data, info.data, info.data_size);
	}

	if(((info.flags) & (tz_gpu_resource_flag.zero_memory)) != zero)
	{
		memfill(resinfo->data, 0, info.data_size);
	}

	// write resource data to gpu memory.
	impl_write_a_resource(resptr);
};

// api

tz_gpu_init ::= func(info : tz_gpu_appinfo -> v0)
{
	TZ_GPU_VERSION ::= VK_MAKE_API_VERSION(0, 0, 1, 0);
	vulkan_init();
	target_window = tz_wnd.invalid;

	appinfo ::= VkApplicationInfo
	{
		.sType := 0;
		.pNext := zero;
		.pApplicationName := info.name;
		.applicationVersion := VK_MAKE_API_VERSION(0, info.ver_maj, info.ver_min, 0);
		.pEngineName := "tz_gpu";
		.engineVersion := TZ_GPU_VERSION;
		.apiVersion := VK_MAKE_API_VERSION(0, 1, 3, 0);
	};

	extensions : u8? mut[3];
	deref(extensions # 0) = "VK_KHR_surface";
	static if(_win32)
	{
		deref(extensions # 1) = "VK_KHR_win32_surface";
	}
	static if(_linux)
	{
		deref(extensions # 1) = "VK_KHR_xlib_surface";
	}
	// todo: make VK_EXT_debug_utils optional.
	deref(extensions # 2) = "VK_EXT_debug_utils";
	extension_count : u32 mut := __sizeof(extensions) / __sizeof(u8?);

	layers : u8? mut[1];
	deref(layers # 0) = "VK_LAYER_KHRONOS_validation";

	//static if(__config() == "debug")
	//{
		layer_count ::= 1;
		putzstr("validation layers enabled");
		putchar(10);
	//}
	//static if(__config() == "release")
	//{
	//	layer_count ::= 0;
	//	extension_count = (extension_count - 1);
	//}

	create ::= VkInstanceCreateInfo
	{
		.sType := 1;
		.pNext := zero;
		.flags := 0;
		.pApplicationInfo := ref appinfo;
		.enabledLayerCount := layer_count;
		.ppEnabledLayerNames := (layers # 0)@_;
		.enabledExtensionCount := extension_count;
		.ppEnabledExtensionNames := (extensions # 0)@_;
	};

	vk_check(vk.create_instance(ref create, zero, ref vkinst));
	vulkan_load_functions(vkinst);
};

tz_gpu_term ::= func( -> v0)
{
	vk.device_wait_idle(used_device);
	if(pipeline_layout != 0)
	{
		vk.destroy_pipeline_layout(used_device, pipeline_layout, zero);
		pipeline_layout = 0;
	}
	if(scratch.cpool != 0)
	{
		vk.destroy_command_pool(used_device, scratch.cpool, zero);
		scratch.cpool = 0;
	}
	if(scratch.fence != 0)
	{
		vk.destroy_fence(used_device, scratch.fence, zero);
	}
	counter : u64 mut;
	frame_ptr : frame_data mut? mut;
	for(counter = 0, counter < frame_overlap, counter = counter + 1)
	{
		frame_ptr = (frames # counter);
		if(frame_ptr->cpool != 0)
		{
			vk.destroy_command_pool(used_device, frame_ptr->cpool, zero);
			(frame_ptr->cpool) = 0;
		}
		if(frame_ptr->swapchain_fence != 0)
		{
			vk.destroy_fence(used_device, frame_ptr->swapchain_fence, zero);
			(frame_ptr->swapchain_fence) = 0;
		}
		if(frame_ptr->swapchain_sem != 0)
		{
			vk.destroy_semaphore(used_device, frame_ptr->swapchain_sem, zero);
			(frame_ptr->swapchain_sem) = 0;
		}

		// dont forget descriptor layouts.
		vk.destroy_descriptor_set_layout(used_device, deref(set_layouts # counter), zero);
		deref(set_layouts # counter) = 0;
	}
	for(counter = 0, counter < (swapchain_image_count@_), counter = counter + 1)
	{
		vk.destroy_image_view(used_device, deref(swapchain_views # counter), zero);
	}
	if(swapchain != 0)
	{
		vk.destroy_swapchain_khr(used_device, swapchain, zero);
	}

	if(system_image_view != 0)
	{
		vk.destroy_image_view(used_device, system_image_view, zero);
		system_image_view = 0;
	}
	if(system_image != 0)
	{
		vk.destroy_image(used_device, system_image, zero);
		system_image = 0;
	}

	// destroy all passes
	passptr : pass_data_t mut? mut;
	for(counter = 0, counter < passes_count, counter = counter + 1)
	{
		passptr = passes # counter;	
		vk.destroy_buffer(used_device, passptr->metabuf, zero);
		vk.destroy_pipeline(used_device, passptr->pipeline, zero);
	}
	passes_count = 0;

	// destroy all resources.
	resptr : resource_data_t mut? mut;
	for(counter = 0, counter < resource_count, counter = counter + 1)
	{
		resptr = resources # counter;	
		if(resptr->is_buffer)
		{
			vk.destroy_buffer(used_device, resptr->vk_handle, zero);
			(resptr->vk_handle) = 0;
		}
		if(!(resptr->is_buffer))
		{
			vk.destroy_sampler(used_device, resptr->sampler, zero);
			vk.destroy_image_view(used_device, resptr->image_view, zero);
			vk.destroy_image(used_device, resptr->vk_handle, zero);
			(resptr->vk_handle) = 0;
		}
	}
	resource_count = 0;

	for(counter = 0, counter < descriptor_pools_count, counter = counter + 1)
	{
		vk.destroy_descriptor_pool(used_device, deref(descriptor_pools_data # counter), zero);
	}
	descriptor_pools_count = 0;

	// destroy all shaders.
	shad : shader_data_t mut? mut;
	for(counter = 0, counter < shaders_count, counter = counter + 1)
	{
		shad = shaders # counter;	
		if(shad->vertex_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->vertex_module, zero);
			shad->vertex_module = 0;
		}
		if(shad->fragment_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->fragment_module, zero);
			shad->fragment_module = 0;
		}
		if(shad->compute_module != 0)
		{
			vk.destroy_shader_module(used_device, shad->compute_module, zero);
			shad->compute_module = 0;
		}
	}
	shaders_count = 0;

	impl_vallocator_free_all();

	vk.destroy_device(used_device, zero);
	if(surface != 0)
	{
		vk.destroy_surface_khr(vkinst, surface, zero);
	}
	vk.destroy_instance(vkinst, zero);
	vulkan_term();
};

tz_gpu_hardware_count ::= func( -> u64)
{
	ret : u32 mut;
	vk_check(vk.enumerate_physical_devices(vkinst, ref ret, zero));
	return ret@_;
};

tz_gpu_iterate_hardware ::= func(array : tz_gpu_hardware mut?, array_size : u64, a : arena mut? -> tz_gpu_err)
{
	// need an array of VkPhysicalDevices for vulkan to write into.
	pdev_storage : u64 mut? := arena_alloc(a, __sizeof(u64) * array_size);
	physical_device_count : u32 mut := array_size@_;
	result ::= vk.enumerate_physical_devices(vkinst, ref physical_device_count, pdev_storage);
	if(result != 0)
	{
		return tz_gpu_err.unknown;
	}
	
	counter : u64 mut;
	for(counter = 0, counter < array_size, counter = counter + 1)
	{
		impl_fill_hardware_info(array # counter, deref(pdev_storage # counter), a);
	}
	return tz_gpu_err.none;
};

tz_gpu_use_hardware ::= func(hardware : tz_gpu_hardware, window : tz_wnd -> v0)
{
	target_window = window;
	pdev ::= hardware.native_handle;
	queue_priority : f32 := 1.0;

	qcreate ::= VkDeviceQueueCreateInfo
	{
		.sType := 2;
		.pNext := zero;
		.flags := 0;
		.queueFamilyIndex := hardware.id;
		.queueCount := 1;
		.pQueuePriorities := ref queue_priority;
	};

	features13 ::= VkPhysicalDeviceVulkan13Features
	{
		.sType := 53;
		.pNext := zero;
		.robustImageAccess := 0;
		.inlineUniformBlock := 0;
		.descriptorBindingInlineUniformBlockUpdateAfterBind := 0;
		.pipelineCreationCacheControl := 0;
		.privateData := 0;
		.shaderDemoteToHelperInvocation := 0;
		.shaderTerminateInvocation := 0;
		.subgroupSizeControl := 0;
		.computeFullSubgroups := 0;
		.synchronization2 := 1;
		.textureCompressionASTC_HDR := 0;
		.shaderZeroInitializeWorkgroupMemory := 0;
		.dynamicRendering := 1;
		.shaderIntegerDotProduct := 0;
		.maintenance4 := 0;
	};

	features12 ::= VkPhysicalDeviceVulkan12Features 
	{
		.sType := 51;
		.pNext := (ref features13)@_;
		.samplerMirrorClampToEdge := 0;
		.drawIndirectCount := 1;
		.storageBuffer8BitAccess := 0;
		.uniformAndStorageBuffer8BitAccess := 0;
		.storagePushConstant8 := 0;
		.shaderBufferInt64Atomics := 0;
		.shaderSharedInt64Atomics := 0;
		.shaderFloat16 := 0;
		.shaderInt8 := 0;
		.descriptorIndexing := 1;
		.shaderInputAttachmentArrayDynamicIndexing := 0;
		.shaderUniformTexelBufferArrayDynamicIndexing := 0;
		.shaderStorageTexelBufferArrayDynamicIndexing := 0;
		.shaderUniformBufferArrayNonUniformIndexing := 0;
		.shaderSampledImageArrayNonUniformIndexing := 1;
		.shaderStorageBufferArrayNonUniformIndexing := 0;
		.shaderStorageImageArrayNonUniformIndexing := 0;
		.shaderInputAttachmentArrayNonUniformIndexing := 0;
		.shaderUniformTexelBufferArrayNonUniformIndexing := 0;
		.shaderStorageTexelBufferArrayNonUniformIndexing := 0;
		.descriptorBindingUniformBufferUpdateAfterBind := 0;
		.descriptorBindingSampledImageUpdateAfterBind := 1;
		.descriptorBindingStorageImageUpdateAfterBind := 0;
		.descriptorBindingStorageBufferUpdateAfterBind := 0;
		.descriptorBindingUniformTexelBufferUpdateAfterBind := 0;
		.descriptorBindingStorageTexelBufferUpdateAfterBind := 0;
		.descriptorBindingUpdateUnusedWhilePending := 1;
		.descriptorBindingPartiallyBound := 1;
		.descriptorBindingVariableDescriptorCount := 1;
		.runtimeDescriptorArray := 1;
		.samplerFilterMinmax := 0;
		.scalarBlockLayout := 1;
		.imagelessFramebuffer := 0;
		.uniformBufferStandardLayout := 0;
		.shaderSubgroupExtendedTypes := 0;
		.separateDepthStencilLayouts := 0;
		.hostQueryReset := 0;
		.timelineSemaphore := 1;
		.bufferDeviceAddress := 1;
		.bufferDeviceAddressCaptureReplay := 0;
		.bufferDeviceAddressMultiDevice := 0;
		.vulkanMemoryModel := 0;
		.vulkanMemoryModelDeviceScope := 0;
		.vulkanMemoryModelAvailabilityVisibilityChains := 0;
		.shaderOutputViewportIndex := 0;
		.shaderOutputLayer := 0;
		.subgroupBroadcastDynamicId := 0;
	};

	features11 ::= VkPhysicalDeviceVulkan11Features 
	{
		.sType := 49;
		.pNext := (ref features12)@_;
		.storageBuffer16BitAccess := 0;
		.uniformAndStorageBuffer16BitAccess := 0;
		.storagePushConstant16 := 0;
		.storageInputOutput16 := 0;
		.multiview := 0;
		.multiviewGeometryShader := 0;
		.multiviewTessellationShader := 0;
		.variablePointersStorageBuffer := 0;
		.variablePointers := 0;
		.protectedMemory := 0;
		.samplerYcbcrConversion := 0;
		.shaderDrawParameters := 1;
	};

	enabled_features ::= VkPhysicalDeviceFeatures2
	{
		.sType := 1000059000;
		.pNext := (ref features11)@_;
		.features := VkPhysicalDeviceFeatures
		{
			.robustBufferAccess := 0;
			.fullDrawIndexUint32 := 0;
			.imageCubeArray := 0;
			.independentBlend := 0;
			.geometryShader := 0;
			.tessellationShader := 0;
			.sampleRateShading := 0;
			.dualSrcBlend := 0;
			.logicOp := 0;
			.multiDrawIndirect := 0;
			.drawIndirectFirstInstance := 0;
			.depthClamp := 0;
			.depthBiasClamp := 0;
			.fillModeNonSolid := 0;
			.depthBounds := 0;
			.wideLines := 0;
			.largePoints := 0;
			.alphaToOne := 0;
			.multiViewport := 0;
			.samplerAnisotropy := 0;
			.textureCompressionETC2 := 0;
			.textureCompressionASTC_LDR := 0;
			.textureCompressionBC := 0;
			.occlusionQueryPrecise := 0;
			.pipelineStatisticsQuery := 0;
			.vertexPipelineStoresAndAtomics := 0;
			.fragmentStoresAndAtomics := 0;
			.shaderTessellationAndGeometryPointSize := 0;
			.shaderImageGatherExtended := 0;
			.shaderStorageImageExtendedFormats := 0;
			.shaderStorageImageMultisample := 0;
			.shaderStorageImageReadWithoutFormat := 0;
			.shaderStorageImageWriteWithoutFormat := 0;
			.shaderUniformBufferArrayDynamicIndexing := 0;
			.shaderSampledImageArrayDynamicIndexing := 0;
			.shaderStorageBufferArrayDynamicIndexing := 0;
			.shaderStorageImageArrayDynamicIndexing := 0;
			.shaderClipDistance := 0;
			.shaderCullDistance := 0;
			.shaderFloat64 := 0;
			.shaderInt64 := 0;
			.shaderInt16 := 0;
			.shaderResourceResidency := 0;
			.shaderResourceMinLod := 0;
			.sparseBinding := 0;
			.sparseResidencyBuffer := 0;
			.sparseResidencyImage2D := 0;
			.sparseResidencyImage3D := 0;
			.sparseResidency2Samples := 0;
			.sparseResidency4Samples := 0;
			.sparseResidency8Samples := 0;
			.sparseResidency16Samples := 0;
			.sparseResidencyAliased := 0;
			.variableMultisampleRate := 0;
			.inheritedQueries := 0;
		};
	};

	extensions : u8? mut[1];
	deref(extensions # 0) = "VK_KHR_swapchain";
	extension_count : u32 := __sizeof(extensions) / __sizeof(u8?);

	create ::= VkDeviceCreateInfo
	{
		.sType := 3;
		.pNext := (ref enabled_features)@_;
		.flags := 0;
		.queueCreateInfoCount := 1;
		.pQueueCreateInfos := (ref qcreate)@_;
		.enabledLayerCount := 0;
		.ppEnabledLayerNames := zero;
		.enabledExtensionCount := extension_count;
		.ppEnabledExtensionNames := (extensions # 0)@_;
		.pEnabledFeatures := zero;
	};

	vk_check(vk.create_device(pdev, ref create, zero, ref used_device));
	used_hardware = pdev;
	used_qfi = (hardware.id);
	used_mti_gpu = (hardware.target_heap_gpu)@_;
	used_mti_cpu = (hardware.target_heap_cpu)@_;

	vk.get_device_queue(used_device, used_qfi, 0, ref graphics_queue);
	vk.get_device_queue(used_device, used_qfi, 0, ref compute_queue);

	pool_create ::= VkCommandPoolCreateInfo
	{
		.sType := 39;
		.pNext := zero;
		.flags := 0x02;
		.queueFamilyIndex := used_qfi;
	};
	
	counter : u64 mut := 0;
	frame_ptr : frame_data mut? mut;

	cmd_info : VkCommandBufferAllocateInfo mut := VkCommandBufferAllocateInfo
	{
		.sType := 40;
		.pNext := zero;
		.commandPool := 0;
		.level := 0;
		.commandBufferCount := 1;
	};

	fence_create ::= VkFenceCreateInfo
	{
		.sType := 8;
		.pNext := zero;
		.flags := 0;
	};

	sem_create ::= VkSemaphoreCreateInfo
	{
		.sType := 9;
		.pNext := zero;
		.flags := 0;
	};

	for(counter = 0, counter < frame_overlap, counter = counter + 1)
	{
		frame_ptr = (frames # counter);
		vk_check(vk.create_command_pool(used_device, ref pool_create, zero, ref (frame_ptr->cpool)));
		cmd_info.commandPool = (frame_ptr->cpool);
		vk_check(vk.allocate_command_buffers(used_device, ref cmd_info, ref (frame_ptr->cmds)));

		vk_check(vk.create_fence(used_device, ref fence_create, zero, ref (frame_ptr->swapchain_fence)));
		vk_check(vk.create_semaphore(used_device, ref sem_create, zero, ref (frame_ptr->swapchain_sem)));
	}

	vk_check(vk.create_command_pool(used_device, ref pool_create, zero, ref (scratch.cpool)));
	cmd_info.commandPool = scratch.cpool;
	vk_check(vk.allocate_command_buffers(used_device, ref cmd_info, ref (scratch.cmds)));

	vk_check(vk.create_fence(used_device, ref fence_create, zero, ref (scratch.fence)));

	pipeline_layout = impl_initialise_pipeline_layout();
	impl_vallocator_initial_setup();

	putzstr("using ");
	putzstr(hardware.name);
	putchar(10);
};

tz_gpu_create_buffer ::= func(info : tz_gpu_resource_info, a : arena mut? -> tz_gpu_resource)
{
	retid ::= resource_count;
	resptr ::= impl_alloc_new_resource(a);
	impl_init_buffer(resptr, info, a);
	return retid@tz_gpu_resource;
};

tz_gpu_create_image ::= func(info : tz_gpu_resource_info, a : arena mut? -> tz_gpu_resource)
{
	retid ::= resource_count;
	resptr ::= impl_alloc_new_resource(a);
	resptr->info = info;
	resptr->is_buffer = false;
	fmt : s32 mut;
	aspect_mask : s32 mut;
	if((info.image_type) == (tz_gpu_image_type.rgba))
	{
		fmt = rgba_format;
		aspect_mask = 0x00000001;
	}
	if((info.image_type) == (tz_gpu_image_type.depth))
	{
		fmt = depth_format;
		aspect_mask = 0x00000002;
	}
	dims ::= info.image_dimensions;
	w ::= deref(dims # 0);
	h ::= deref(dims # 1);
	create : VkImageCreateInfo mut := VkImageCreateInfo
	{
		.sType := 14;
		.pNext := zero;
		.flags := 0;
		.imageType := 1;
		.format := fmt;
		.extent := VkExtent3D
		{
			.width := w;
			.height := h;
			.depth := 1;
		};
		.mipLevels := 1;
		.arrayLayers := 1;
		.samples := 1;
		.tiling := 0;
		.usage := (0x00000004 | 0x00000002); // VK_IMAGE_USAGE_SAMPLED_BIT | VK_IMAGE_USAGE_TRANSFER_DST_BIT 
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
		.initialLayout := 0;
	};
	vk_check(vk.create_image(used_device, ref create, zero, ref (resptr->vk_handle)));
	impl_bind_image_mem(resptr->vk_handle, w * h * 4);

	view_create ::= VkImageViewCreateInfo
	{
		.sType := 15;
		.pNext := zero;
		.flags := 0;
		.image := resptr->vk_handle;
		.viewType := 1;
		.format := fmt;
		.components := VkComponentMapping
		{
			.r := 0;
			.g := 0;
			.b := 0;
			.a := 0;
		};
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := aspect_mask;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};

	vk_check(vk.create_image_view(used_device, ref view_create, zero, ref (resptr->image_view)));

	sampler_create ::= VkSamplerCreateInfo
	{
		.sType := 31;
		.pNext := zero;
		.flags := 0;
		.magFilter := 0;
		.minFilter := 0;
		.mipmapMode := 0;
		.addressModeU := 2;
		.addressModeV := 2;
		.addressModeW := 2;
		.mipLodBias := 0.0;
		.anisotropyEnable := 0;
		.maxAnisotropy := 0.0;
		.compareEnable := 0;
		.compareOp := 7;
		.minLod := 0.0;
		.maxLod := 0.0;
		.borderColor := 3;
		.unnormalizedCoordinates := 0;
	};
	vk_check(vk.create_sampler(used_device, ref sampler_create, zero, ref (resptr->sampler)));

	// make a copy of the resource data using the arena.
	resinfo ::= ref(resptr->info);
	resinfo->data = arena_alloc(a, info.data_size);
	if(info.data != zero)
	{
		memcopy(resinfo->data, info.data, info.data_size);
	}

	if(((info.flags) & (tz_gpu_resource_flag.zero_memory)) != zero)
	{
		memfill(resinfo->data, 0, info.data_size);
	}

	// write resource data to gpu memory.
	impl_write_a_resource(resptr);

	return retid@tz_gpu_resource;
};

tz_gpu_load_shader_files ::= func(vertex_spv_path : u8?, fragment_spv_path : u8?, a : arena mut? -> tz_gpu_shader_sources)
{
	ret : tz_gpu_shader_sources mut;
	if(!file_exists(vertex_spv_path))
	{
		putzstr("missing vertex spv");
		__debugbreak();
	}
	if(!file_exists(fragment_spv_path))
	{
		putzstr("missing fragment spv");
		__debugbreak();
	}
	ret.vertex_spv_count = file_size_bytes(vertex_spv_path);
	ret.fragment_spv_count = file_size_bytes(fragment_spv_path);

	ret.vertex_spv_data = arena_alloc(a, ret.vertex_spv_count);
	ret.fragment_spv_data = arena_alloc(a, ret.fragment_spv_count);

	file_read(vertex_spv_path, ret.vertex_spv_data, ret.vertex_spv_count);
	file_read(fragment_spv_path, ret.fragment_spv_data, ret.fragment_spv_count);

	return ret;
};

tz_gpu_create_graphics_shader ::= func(vertex_source : u8?, vertex_source_len : u64, fragment_source : u8?, fragment_source_len : u64, a : arena mut? -> tz_gpu_shader)
{
	retid ::= shaders_count;
	resptr ::= impl_alloc_new_shader(a);
	resptr->is_graphics = true;
	resptr->compute_module = 0;

	vcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := zero;
		.flags := 0;
		.codeSize := vertex_source_len@_;
		.pCode := vertex_source@_;
	};
	vk.create_shader_module(used_device, ref vcreate, zero, ref (resptr->vertex_module));
	fcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := zero;
		.flags := 0;
		.codeSize := fragment_source_len@_;
		.pCode := fragment_source@_;
	};
	vk.create_shader_module(used_device, ref fcreate, zero, ref (resptr->fragment_module));

	return retid@tz_gpu_shader;
};

tz_gpu_create_compute_shader ::= func(compute_source : u8?, compute_source_len : u64, a : arena mut? -> tz_gpu_shader)
{
	retid ::= shaders_count;
	resptr ::= impl_alloc_new_shader(a);
	resptr->is_graphics = false;
	resptr->vertex_module = 0;
	resptr->fragment_module = 0;

	vcreate ::= VkShaderModuleCreateInfo
	{
		.sType := 16;
		.pNext := zero;
		.flags := 0;
		.codeSize := compute_source_len@_;
		.pCode := compute_source@_;
	};
	vk.create_shader_module(used_device, ref vcreate, zero, ref (resptr->compute_module));

	return retid@tz_gpu_shader;
};

tz_gpu_create_pass ::= func(info : tz_gpu_pass_info, long : arena mut?, short : arena mut? -> tz_gpu_pass)
{
	retid ::= passes_count;
	passptr ::= impl_alloc_new_pass(long);
	passptr->info = info;
	// first thing we need is a metabuffer.
	// this is a non-BDA buffer that contains all the BDA addresses.
	// let's figure out how many buffer resources we have
	buffer_rescount : u64 mut := 0;
	counter : u64 mut;
	cur_resource : resource_data_t mut;
	for(counter = 0, counter < (info.resources_count), counter = counter + 1)
	{
		cur_resource = deref (resources # (deref(info.resources_data # counter)@s64));
		if(cur_resource.is_buffer)
		{
			buffer_rescount = buffer_rescount + 1;
		}
	}

	size : u64 mut := 1;
	if(buffer_rescount > 0)
	{
		size = buffer_rescount * __sizeof(u64);
	}
	meta_create ::= VkBufferCreateInfo
	{
		.sType := 12;
		.pNext := zero;
		.flags := 0;
		.size := size;
		.usage := (0x00000020 | 0x00000002); // VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT 
		.sharingMode := 0;
		.queueFamilyIndexCount := 1;
		.pQueueFamilyIndices := ref used_qfi;
	};
	vk_check(vk.create_buffer(used_device, ref meta_create, zero, ref (passptr->metabuf)));
	(passptr->metabuf_size) = size;
	impl_bind_buffer_mem(passptr->metabuf, size, true);
	// create pipeline.
	if(passptr->is_compute)
	{
		passptr->pipeline = impl_create_compute_pipeline(info.shader, info.compute);
	}

	passptr->targets_swapchain = false;

	passptr->is_compute = impl_shader_is_compute(info.shader);
	ginfo ::= info.graphics;
	cur_colour_target : tz_gpu_resource mut;
	if(!(passptr->is_compute))
	{
		if((ginfo.draw_buffer) != (tz_gpu_resource.invalid))
		{
			drawbuf_handle ::= (ginfo.draw_buffer)@s64;
			drawbuf : resource_data_t? := resources # drawbuf_handle;
			dbinfo ::= drawbuf->info;
			dbflags ::= dbinfo.buffer_flags;
			if((dbflags & (tz_gpu_buffer_flag.draw_buffer)) != zero)
			{
				putzstr("error: draw_buffer (");
				putuint(drawbuf_handle);
				putzstr(") passed to graphics renderer ");
				putchar('"');
				putzstr(info.name);
				putchar('"');
				putzstr(" did not have tz_gpu_buffer_flag.draw_buffer.");
				__debugbreak();
			}
		}

		passptr->pipeline = impl_create_graphics_pipeline(info.shader, info.graphics, long);
		for(counter = 0, counter < (ginfo.colour_targets_count), counter = counter + 1)
		{
			cur_colour_target = deref((ginfo.colour_targets_data) # counter);
			if(cur_colour_target == (tz_gpu_resource.window_resource))
			{
				impl_require_swapchain(long);
				passptr->targets_swapchain = true;
			}
		}

		(passptr->colour_target_dimensions) = impl_get_pass_colour_target_dimensions(passptr);
	}

	impl_write_resources(passptr, long, short);
	impl_populate_descriptors(passptr, long, short);

	return retid@tz_gpu_pass;
};

tz_gpu_create_graph ::= func(name : u8?, a : arena mut? -> tz_gpu_graph)
{
	ret ::= graph_count;
	graphptr ::= impl_alloc_new_graph(a);
	namelen ::= zstrlen(name);
	graphptr->name = arena_alloc(a, namelen + 1);
	memcopy(graphptr->name, name, namelen);
	deref((graphptr->name) # namelen) = 0;

	initial_timeline_capacity ::= 8;
	graphptr->timeline = arena_alloc(a, __sizeof(graph_entry) * initial_timeline_capacity);
	graphptr->timeline_count = 0;
	graphptr->timeline_cap = initial_timeline_capacity;
	return ret@tz_gpu_graph;
};

tz_gpu_graph_add_pass ::= func(graph : tz_gpu_graph, pass : tz_gpu_pass -> v0)
{
	graphptr ::= graphs # (graph@s64);
	if(((graphptr->timeline_count) + 1) > (graphptr->timeline_cap))
	{
		putzstr("graph ran out of entries in timeline. todo: expand timeline array");
		__debugbreak();
	}
	cur ::= graphptr->timeline_count;
	entry_ptr ::= (graphptr->timeline) # cur;
	(entry_ptr->handle) = (pass@s64@_);
	entry_ptr->is_graph = false;
	(graphptr->timeline_count) = (graphptr->timeline_count) + 1;
};

// todo: take in render graph and go through the passes in that order instead.
tz_gpu_execute ::= func(graph : tz_gpu_graph, long : arena mut?, short : arena mut? -> v0)
{
	graphptr ::= graphs # (graph@s64);
	writes_to_system_image ::= impl_graph_writes_to_system_image(graph);
	will_present ::= impl_graph_will_present(graph);

	frame ::= deref(frames # current_frame);


	image_index : u32 mut := -1@u32;
	swapchain_image : u64 mut := 0;
	if(will_present)
	{
		// make sure swapchain is available.
		impl_require_swapchain(long);
		if(swapchain_width == 0)
		{
			return;
		}
		if(swapchain_height == 0)
		{
			return;
		}

		vk_check(vk.acquire_next_image_khr(used_device, swapchain, 9999999999, 0, frame.swapchain_fence, ref image_index));
		vk_check(vk.wait_for_fences(used_device, 1, ref (frame.swapchain_fence), 1, ~0));
		vk_check(vk.reset_fences(used_device, 1, ref (frame.swapchain_fence)));
		swapchain_image = deref(swapchain_images # image_index);
	}

	barrier : VkImageMemoryBarrier mut := VkImageMemoryBarrier
	{
		.sType := 45;
		.pNext := zero;
		.srcAccessMask := 0;
		.dstAccessMask := 0x00001000;//VK_ACCESS_TRANSFER_WRITE_BIT
		.oldLayout := 0;
		.newLayout := 7; // VK_IMAGE_LAYOUT_TRANSFER_DST_OPTIMAL
		.srcQueueFamilyIndex := ~0;
		.dstQueueFamilyIndex := ~0;
		.image := 0;
		.subresourceRange := VkImageSubresourceRange
		{
			.aspectMask := 0x00000001;
			.baseMipLevel := 0;
			.levelCount := 1;
			.baseArrayLayer := 0;
			.layerCount := 1;
		};
	};
	if(will_present)
	{
		(barrier.image) = swapchain_image;
	}

	frame_begin ::= VkCommandBufferBeginInfo
	{
		.sType := 42;
		.pNext := zero;
		.flags := 0;
		.pInheritanceInfo := zero;
	};
	vk_check(vk.begin_command_buffer(frame.cmds, ref frame_begin));

	if(will_present)
	{
		vk.cmd_pipeline_barrier(frame.cmds, 0x00002000, 0x00001000, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	counter : u64 mut;
	cur_entry : graph_entry mut;
	for(counter = 0, counter < (graphptr->timeline_count), counter = counter + 1)
	{
		cur_entry = deref((graphptr->timeline) # counter);
		if(cur_entry.is_graph)
		{
			putzstr("subgraphs are not yet implemented");
			__debugbreak();
		}
		impl_record_gpu_work((cur_entry.handle)@s64@tz_gpu_pass, current_frame, short);
	}

	// if we just wrote to the system image:
	// 	1. system image must be in color attachment layout. to eventually present later on, we should now transition to transfer_src (to transfer it to swapchain image).
	// 	2. record a command to do the blit (swapchain image should be transfer_dst)
	// 	3. transition the swapchain image to present_src.
	if(writes_to_system_image)
	{
		blit : VkImageBlit mut := VkImageBlit
		{
			.srcSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.srcOffsets := zero;
			.dstSubresource := VkImageSubresourceLayers
			{
				.aspectMask := 0x00000001;
				.mipLevel := 0;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
			.dstOffsets := zero;
		};
		deref((blit.srcOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.srcOffsets) # 1) = VkOffset3D{.x := swapchain_width@_; .y := swapchain_height@_; .z := 1;};
		deref((blit.dstOffsets) # 0) = VkOffset3D{.x := 0; .y := 0; .z := 0;};
		deref((blit.dstOffsets) # 1) = VkOffset3D{.x := swapchain_width@_; .y := swapchain_height@_; .z := 1;};

		system_image_transition ::= VkImageMemoryBarrier
		{
			.sType := 45;
			.pNext := zero;
			.srcAccessMask := 0x00000100; // VK_ACCESS_COLOR_ATTACHMENT_WRITE_BIT 
			.dstAccessMask := 0x00000800; // VK_ACCESS_TRANSFER_READ_BIT
			.oldLayout := 2; // VK_IMAGE_LAYOUT_COLOR_ATTACHMENT_OPTIMAL
			.newLayout := 6; // VK_IMAGE_LAYOUT_TRANSFER_SRC_OPTIMAL
			.srcQueueFamilyIndex := ~0;
			.dstQueueFamilyIndex := ~0;
			.image := system_image;
			.subresourceRange := VkImageSubresourceRange
			{
				.aspectMask := 0x00000001;
				.baseMipLevel := 0;
				.levelCount := 1;
				.baseArrayLayer := 0;
				.layerCount := 1;
			};
		};

		vk.cmd_pipeline_barrier(frame.cmds, 0x00000400, 0x00001000, 0, 0, zero, 0, zero, 1, ref system_image_transition);
		vk.cmd_blit_image(frame.cmds, system_image, 6, swapchain_image, 7, 1, ref blit, 0);
	}

	if(will_present)
	{
		barrier.oldLayout = 7;
		barrier.newLayout = 1000001002;
		barrier.srcAccessMask = 0x00001000;
		barrier.dstAccessMask = 0;
		vk.cmd_pipeline_barrier(frame.cmds, 0x00001000, 0x00000001, 0, 0, zero, 0, zero, 1, ref barrier);
	}

	vk_check(vk.end_command_buffer(frame.cmds));
	wait_stage : s32 := 0;
	submit ::= VkSubmitInfo
	{
		.sType := 4;
		.pNext := zero;
		.waitSemaphoreCount := 0;
		.pWaitSemaphores := zero;
		.pWaitDstStageMask := ref wait_stage;
		.commandBufferCount := 1;
		.pCommandBuffers := ref (frame.cmds);
		.signalSemaphoreCount := 1;
		.pSignalSemaphores := ref (frame.swapchain_sem);
	};
	vk_check(vk.queue_submit(graphics_queue, 1, ref submit, frame.swapchain_fence));
	vk_check(vk.wait_for_fences(used_device, 1, ref (frame.swapchain_fence), 1, -1@u64));
	vk_check(vk.reset_fences(used_device, 1, ref (frame.swapchain_fence)));

	// present if we need to.
	present_res : s32 mut;
	if(will_present)
	{
		present ::= VkPresentInfoKHR
		{
			.sType := 1000001001;
			.pNext := zero;
			.waitSemaphoreCount := 1;
			.pWaitSemaphores := ref (frame.swapchain_sem);
			.swapchainCount := 1;
			.pSwapchains := ref swapchain;
			.pImageIndices := ref image_index;
			.pResults := ref present_res;
		};
		vk_check(vk.queue_present_khr(graphics_queue, ref present));
		vk_check(present_res);
	}

	current_frame = ((current_frame + 1) % 2);
};

tz_gpu_resource_write ::= func(res : tz_gpu_resource, data : v0? weak, data_size : u64, offset : u64 -> v0)
{
	vk.device_wait_idle(used_device);
	resptr ::= (resources # (res@s64));
	rinfo ::= ref (resptr->info);

	ptr ::= (rinfo->data)@u8 mut? mut;

	memcopy(ptr # offset, data, data_size);
	impl_write_a_resource(resptr);
};

tz_gpu_resource_size ::= func(res : tz_gpu_resource -> u64)
{
	resptr ::= resources # (res@s64);
	rinfo ::= resptr->info;
	return rinfo.data_size;
};

tz_gpu_resize_buffer ::= func(res : tz_gpu_resource, new_size : u64, long : arena mut?, short : arena mut? -> v0)
{
	resptr ::= resources # (res@s64);
	oldinfo ::= resptr->info;
	olddata ::= oldinfo.data;
	olddatasize ::= oldinfo.data_size;
	if(olddatasize == new_size)
	{
		return;
	}

	vk.device_wait_idle(used_device);
	vk.destroy_buffer(used_device, resptr->vk_handle, zero);

	if(new_size > olddatasize)
	{
		// ok a bit of trouble. old data is too small. need to realloc to be large enough.
		oldinfo.data = arena_alloc(long, new_size);
		oldinfo.data_size = new_size;

		// preserve old data
		memcopy(oldinfo.data, olddata, olddatasize);
		// fill the rest with zeros.
		offset_ptr ::= (oldinfo.data)@u8 mut? mut;
		offset_ptr = offset_ptr # olddatasize;
		memfill(offset_ptr, 0, new_size - olddatasize);
	}

	impl_init_buffer(resptr, oldinfo, long);

	// problem is, all the passes that use this buffer as a resource will need to have their metabuffer written to.
	// this is because the metabuffer will contain the BDA of the old buffer which is now out of date.
	// solution:
	// for all passes, if they contained this buffer, rewrite their metabuffers.
	counter : u64 mut;
	for(counter = 0, counter < passes_count, counter = counter + 1)
	{
		if(impl_pass_uses_resource(counter@s64@tz_gpu_pass, res))
		{
			impl_write_resources(passes # counter, long, short);
		}
	}
};

== build ==
{
	add_source_file("stdlib/file.psy");

	add_source_file("vulkan.psy");
	add_source_file("tz_wnd.psy");
}
